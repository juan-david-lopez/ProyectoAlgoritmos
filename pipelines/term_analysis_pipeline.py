"""
Term Analysis Pipeline - An√°lisis Completo Integrado
Integra las 3 partes del proyecto:
- Parte 1: An√°lisis de t√©rminos predefinidos
- Parte 2: Extracci√≥n autom√°tica de t√©rminos
- Parte 3: Evaluaci√≥n de precisi√≥n
"""

import os
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# Importar componentes del proyecto
from predefined_term_analyzer import PredefinedTermAnalyzer
from auto_term_extractor import AutoTermExtractor
from term_precision_evaluator import TermPrecisionEvaluator


class TermAnalysisPipeline:
    """
    Pipeline completo para an√°lisis de t√©rminos en corpus acad√©mico.
    Integra an√°lisis de t√©rminos predefinidos, extracci√≥n autom√°tica y evaluaci√≥n.
    """

    def __init__(self, unified_data_path: str, output_dir: str):
        """
        Inicializa el pipeline.

        Args:
            unified_data_path: Ruta al archivo unified_abstracts.json
            output_dir: Directorio para guardar todos los outputs
        """
        self.unified_data_path = unified_data_path
        self.output_dir = output_dir

        # Crear directorio de salida
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        # Subdirectorios
        self.viz_dir = os.path.join(output_dir, 'visualizations')
        self.data_dir = os.path.join(output_dir, 'data')
        self.reports_dir = os.path.join(output_dir, 'reports')

        for dir_path in [self.viz_dir, self.data_dir, self.reports_dir]:
            Path(dir_path).mkdir(parents=True, exist_ok=True)

        # Datos y resultados
        self.data = None
        self.abstracts = None
        self.predefined_terms = None
        self.predefined_results = None
        self.extracted_results = None
        self.evaluation_results = None

    def load_data(self) -> Dict:
        """
        Carga datos del archivo unified_abstracts.json.

        Returns:
            Diccionario con datos del unified file
        """
        print("\n" + "="*70)
        print("CARGANDO DATOS")
        print("="*70)

        with open(self.unified_data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        # Extraer abstracts
        self.abstracts = [
            paper['abstract']
            for paper in self.data['papers']
            if paper.get('abstract')
        ]

        # Extraer t√©rminos predefinidos
        self.predefined_terms = self.data.get('predefined_terms', [])

        print(f"‚úì Total papers: {len(self.data['papers'])}")
        print(f"‚úì Abstracts disponibles: {len(self.abstracts)}")
        print(f"‚úì T√©rminos predefinidos: {len(self.predefined_terms)}")
        print(f"‚úì Rango de fechas: {self.data['metadata']['date_range']['start']} - "
              f"{self.data['metadata']['date_range']['end']}")

        return self.data

    def analyze_predefined_terms(self) -> Dict:
        """
        Ejecuta an√°lisis de t√©rminos predefinidos (Parte 1).

        Returns:
            Resultados del an√°lisis de t√©rminos predefinidos
        """
        print("\n" + "="*70)
        print("PARTE 1: AN√ÅLISIS DE T√âRMINOS PREDEFINIDOS")
        print("="*70)

        analyzer = PredefinedTermAnalyzer(
            abstracts=self.abstracts,
            predefined_terms=self.predefined_terms
        )

        # Calcular frecuencias
        print("\n[1/3] Calculando frecuencias de t√©rminos...")
        frequencies = analyzer.calculate_term_frequencies()

        # Calcular co-ocurrencias
        print("[2/3] Calculando co-ocurrencias...")
        co_occurrences = analyzer.calculate_co_occurrences()

        # Generar reporte
        print("[3/3] Generando reporte de t√©rminos predefinidos...")
        report_path = os.path.join(self.reports_dir, 'predefined_terms_report.md')
        analyzer.generate_report(report_path)

        # Guardar frecuencias en CSV
        freq_df = pd.DataFrame([
            {'term': term, 'frequency': freq}
            for term, freq in frequencies.items()
        ]).sort_values('frequency', ascending=False)

        freq_csv_path = os.path.join(self.data_dir, 'predefined_terms_frequencies.csv')
        freq_df.to_csv(freq_csv_path, index=False)

        self.predefined_results = {
            'frequencies': frequencies,
            'co_occurrences': co_occurrences,
            'report_path': report_path,
            'freq_csv_path': freq_csv_path,
            'top_10_terms': freq_df.head(10)['term'].tolist()
        }

        print(f"\n‚úì Reporte generado: {report_path}")
        print(f"‚úì Frecuencias guardadas: {freq_csv_path}")

        return self.predefined_results

    def extract_terms_automatically(self) -> Dict:
        """
        Ejecuta extracci√≥n autom√°tica de t√©rminos (Parte 2).

        Returns:
            Resultados de extracci√≥n autom√°tica
        """
        print("\n" + "="*70)
        print("PARTE 2: EXTRACCI√ìN AUTOM√ÅTICA DE T√âRMINOS")
        print("="*70)

        extractor = AutoTermExtractor(self.abstracts)

        # Extraer con RAKE
        print("\n[1/4] Extrayendo t√©rminos con RAKE...")
        extractor.extract_with_rake()
        rake_terms = extractor.rake_terms[:50]
        print(f"  ‚úì RAKE extrajo {len(extractor.rake_terms)} t√©rminos (top 50 seleccionados)")

        # Extraer con TextRank
        print("[2/4] Extrayendo t√©rminos con TextRank...")
        extractor.extract_with_textrank()
        textrank_terms = extractor.textrank_terms[:50]
        print(f"  ‚úì TextRank extrajo {len(extractor.textrank_terms)} t√©rminos (top 50 seleccionados)")

        # Combinar t√©rminos
        print("[3/4] Combinando t√©rminos de ambos m√©todos...")
        combined_terms = extractor.get_combined_top_terms(n=50)
        print(f"  ‚úì T√©rminos combinados: {len(combined_terms)}")

        # Generar reporte
        print("[4/4] Generando reporte de extracci√≥n...")
        extraction_report_path = os.path.join(
            self.reports_dir,
            'extracted_terms_report.md'
        )
        extractor.generate_extraction_report(extraction_report_path)

        # Guardar t√©rminos extra√≠dos en CSV
        extracted_df = pd.DataFrame([
            {
                'term': term,
                'method': method,
                'score': score
            }
            for term, method, score in [
                *[(t[0], 'RAKE', t[1]) for t in rake_terms[:50]],
                *[(t[0], 'TextRank', t[1]) for t in textrank_terms[:50]],
                *[(t, 'Combined', 1.0) for t in combined_terms]
            ]
        ])

        extracted_csv_path = os.path.join(
            self.data_dir,
            'extracted_terms_all_methods.csv'
        )
        extracted_df.to_csv(extracted_csv_path, index=False)

        self.extracted_results = {
            'rake_terms': [t[0] for t in rake_terms],
            'textrank_terms': [t[0] for t in textrank_terms],
            'combined_terms': combined_terms,
            'report_path': extraction_report_path,
            'csv_path': extracted_csv_path,
            'extractor': extractor
        }

        print(f"\n‚úì Reporte generado: {extraction_report_path}")
        print(f"‚úì T√©rminos guardados: {extracted_csv_path}")

        return self.extracted_results

    def evaluate_precision(self) -> Dict:
        """
        Eval√∫a precisi√≥n de t√©rminos extra√≠dos (Parte 3).

        Returns:
            Resultados de evaluaci√≥n de precisi√≥n
        """
        print("\n" + "="*70)
        print("PARTE 3: EVALUACI√ìN DE PRECISI√ìN")
        print("="*70)

        results = {}

        # Evaluar cada m√©todo
        methods = [
            ('RAKE', self.extracted_results['rake_terms']),
            ('TextRank', self.extracted_results['textrank_terms']),
            ('Combined', self.extracted_results['combined_terms'])
        ]

        for method_name, extracted_terms in methods:
            print(f"\n[Evaluando {method_name}]")

            evaluator = TermPrecisionEvaluator(
                self.predefined_terms,
                extracted_terms
            )

            # Calcular m√©tricas
            evaluator.calculate_similarity_matrix()
            matches = evaluator.identify_matches(threshold=0.70)
            metrics = evaluator.calculate_metrics()

            # Generar reporte
            report_path = os.path.join(
                self.reports_dir,
                f'evaluation_{method_name.lower()}.md'
            )
            evaluator.generate_evaluation_report(report_path, self.abstracts)

            results[method_name] = {
                'metrics': metrics,
                'matches': matches,
                'report_path': report_path,
                'evaluator': evaluator
            }

            print(f"  Precision: {metrics['precision']:.2%}")
            print(f"  Recall:    {metrics['recall']:.2%}")
            print(f"  F1-Score:  {metrics['f1_score']:.2%}")

        self.evaluation_results = results

        # Guardar m√©tricas en JSON
        metrics_json_path = os.path.join(self.data_dir, 'evaluation_metrics.json')

        metrics_data = {
            method: {
                'precision': results[method]['metrics']['precision'],
                'recall': results[method]['metrics']['recall'],
                'f1_score': results[method]['metrics']['f1_score'],
                'coverage': results[method]['metrics']['coverage'],
                'n_exact_matches': results[method]['metrics']['n_exact_matches'],
                'n_partial_matches': results[method]['metrics']['n_partial_matches'],
                'n_novel_terms': results[method]['metrics']['n_novel_terms']
            }
            for method in results.keys()
        }

        with open(metrics_json_path, 'w', encoding='utf-8') as f:
            json.dump(metrics_data, f, indent=2)

        print(f"\n‚úì M√©tricas guardadas: {metrics_json_path}")

        return results

    def create_comparative_visualizations(self):
        """
        Crea visualizaciones comparativas entre m√©todos.
        """
        print("\n" + "="*70)
        print("GENERANDO VISUALIZACIONES COMPARATIVAS")
        print("="*70)

        # 1. Comparaci√≥n de m√©tricas entre m√©todos
        self._create_metrics_comparison_chart()

        # 2. Distribuci√≥n de frecuencias de t√©rminos predefinidos
        self._create_frequency_distribution_chart()

        # 3. Overlap entre m√©todos (Venn de 3 conjuntos)
        self._create_methods_overlap_chart()

        # 4. Top t√©rminos por m√©todo
        self._create_top_terms_comparison()

        print("\n‚úì Todas las visualizaciones generadas")

    def _create_metrics_comparison_chart(self):
        """Gr√°fico comparativo de m√©tricas P/R/F1 entre m√©todos."""
        methods = list(self.evaluation_results.keys())
        metrics_names = ['Precision', 'Recall', 'F1-Score']

        data = {
            metric: [
                self.evaluation_results[method]['metrics'][metric.lower().replace('-', '_')]
                for method in methods
            ]
            for metric in metrics_names
        }

        # Crear gr√°fico de barras agrupadas
        x = np.arange(len(methods))
        width = 0.25

        fig, ax = plt.subplots(figsize=(12, 6))

        for i, metric in enumerate(metrics_names):
            offset = width * (i - 1)
            bars = ax.bar(
                x + offset,
                data[metric],
                width,
                label=metric,
                alpha=0.8
            )

            # A√±adir valores encima de las barras
            for bar in bars:
                height = bar.get_height()
                ax.text(
                    bar.get_x() + bar.get_width()/2.,
                    height,
                    f'{height:.2%}',
                    ha='center',
                    va='bottom',
                    fontsize=9
                )

        ax.set_xlabel('M√©todo de Extracci√≥n', fontsize=12, fontweight='bold')
        ax.set_ylabel('Score', fontsize=12, fontweight='bold')
        ax.set_title('Comparaci√≥n de M√©tricas entre M√©todos de Extracci√≥n',
                     fontsize=14, fontweight='bold', pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels(methods)
        ax.legend(fontsize=10)
        ax.set_ylim(0, 1.1)
        ax.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        path = os.path.join(self.viz_dir, 'metrics_comparison.png')
        plt.savefig(path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"  ‚úì Comparaci√≥n de m√©tricas: {path}")

    def _create_frequency_distribution_chart(self):
        """Distribuci√≥n de frecuencias de t√©rminos predefinidos."""
        frequencies = list(self.predefined_results['frequencies'].values())
        frequencies.sort(reverse=True)

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

        # Gr√°fico 1: Top 15 t√©rminos
        top_15 = sorted(
            self.predefined_results['frequencies'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:15]

        terms_15 = [t[0] for t in top_15]
        freqs_15 = [t[1] for t in top_15]

        bars = ax1.barh(range(len(terms_15)), freqs_15, color='steelblue', alpha=0.7)
        ax1.set_yticks(range(len(terms_15)))
        ax1.set_yticklabels(terms_15, fontsize=9)
        ax1.set_xlabel('Frecuencia', fontsize=11, fontweight='bold')
        ax1.set_title('Top 15 T√©rminos Predefinidos', fontsize=12, fontweight='bold')
        ax1.invert_yaxis()
        ax1.grid(axis='x', alpha=0.3)

        # A√±adir valores
        for i, (bar, freq) in enumerate(zip(bars, freqs_15)):
            ax1.text(freq, i, f' {freq}', va='center', fontsize=8)

        # Gr√°fico 2: Distribuci√≥n general
        ax2.hist(frequencies, bins=30, color='coral', alpha=0.7, edgecolor='black')
        ax2.set_xlabel('Frecuencia', fontsize=11, fontweight='bold')
        ax2.set_ylabel('N√∫mero de T√©rminos', fontsize=11, fontweight='bold')
        ax2.set_title('Distribuci√≥n de Frecuencias', fontsize=12, fontweight='bold')
        ax2.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        path = os.path.join(self.viz_dir, 'frequency_distribution.png')
        plt.savefig(path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"  ‚úì Distribuci√≥n de frecuencias: {path}")

    def _create_methods_overlap_chart(self):
        """Diagrama de overlap entre m√©todos de extracci√≥n."""
        from matplotlib_venn import venn3

        rake_set = set(self.extracted_results['rake_terms'])
        textrank_set = set(self.extracted_results['textrank_terms'])
        predefined_set = set(self.predefined_terms)

        fig, ax = plt.subplots(figsize=(10, 8))

        venn = venn3(
            [rake_set, textrank_set, predefined_set],
            set_labels=('RAKE', 'TextRank', 'Predefinidos'),
            ax=ax
        )

        # Personalizar colores
        if venn.get_patch_by_id('100'):
            venn.get_patch_by_id('100').set_color('#ff9999')
        if venn.get_patch_by_id('010'):
            venn.get_patch_by_id('010').set_color('#99ccff')
        if venn.get_patch_by_id('001'):
            venn.get_patch_by_id('001').set_color('#99ff99')

        plt.title('Overlap entre M√©todos de Extracci√≥n y T√©rminos Predefinidos',
                  fontsize=14, fontweight='bold', pad=20)

        path = os.path.join(self.viz_dir, 'methods_overlap.png')
        plt.tight_layout()
        plt.savefig(path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"  ‚úì Overlap entre m√©todos: {path}")

    def _create_top_terms_comparison(self):
        """Tabla visual de top t√©rminos por m√©todo."""
        fig, axes = plt.subplots(1, 3, figsize=(16, 8))

        methods = [
            ('RAKE', self.extracted_results['rake_terms'][:10]),
            ('TextRank', self.extracted_results['textrank_terms'][:10]),
            ('Predefinidos\n(Top)', self.predefined_results['top_10_terms'])
        ]

        for ax, (method_name, terms) in zip(axes, methods):
            # Crear tabla
            table_data = [[f"{i+1}. {term}"] for i, term in enumerate(terms)]

            ax.axis('tight')
            ax.axis('off')

            table = ax.table(
                cellText=table_data,
                cellLoc='left',
                loc='center',
                colWidths=[0.9]
            )

            table.auto_set_font_size(False)
            table.set_fontsize(9)
            table.scale(1, 2)

            # Estilizar celdas
            for i in range(len(table_data)):
                cell = table[(i, 0)]
                cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')
                cell.set_edgecolor('#cccccc')

            ax.set_title(f'Top 10 - {method_name}',
                        fontsize=12, fontweight='bold', pad=10)

        plt.suptitle('Comparaci√≥n de Top 10 T√©rminos por M√©todo',
                     fontsize=14, fontweight='bold', y=0.98)

        path = os.path.join(self.viz_dir, 'top_terms_comparison.png')
        plt.tight_layout()
        plt.savefig(path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"  ‚úì Comparaci√≥n de top t√©rminos: {path}")

    def generate_master_report(self):
        """
        Genera reporte maestro consolidando todos los an√°lisis.
        """
        print("\n" + "="*70)
        print("GENERANDO REPORTE MAESTRO")
        print("="*70)

        report_path = os.path.join(self.reports_dir, 'term_analysis_report.md')

        content = self._build_master_report_content()

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"\n‚úì Reporte maestro generado: {report_path}")

        return report_path

    def _build_master_report_content(self) -> str:
        """Construye contenido del reporte maestro."""

        # Metadata
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        content = f"""# Reporte de An√°lisis Completo de T√©rminos

**Fecha de generaci√≥n**: {timestamp}
**Corpus**: {self.data['metadata']['query']}
**Rango temporal**: {self.data['metadata']['date_range']['start']} - {self.data['metadata']['date_range']['end']}

---

## Resumen Ejecutivo

### Estad√≠sticas del Corpus

| M√©trica | Valor |
|---------|-------|
| Total de Papers | {len(self.data['papers'])} |
| Abstracts Analizados | {len(self.abstracts)} |
| T√©rminos Predefinidos | {len(self.predefined_terms)} |
| Periodo | {self.data['metadata']['date_range']['start']} - {self.data['metadata']['date_range']['end']} |

### Comparaci√≥n de M√©todos de Extracci√≥n

"""

        # Tabla comparativa de m√©tricas
        content += "| M√©todo | Precision | Recall | F1-Score | Coverage |\n"
        content += "|--------|-----------|--------|----------|----------|\n"

        for method in ['RAKE', 'TextRank', 'Combined']:
            metrics = self.evaluation_results[method]['metrics']
            content += (f"| **{method}** | {metrics['precision']:.2%} | "
                       f"{metrics['recall']:.2%} | {metrics['f1_score']:.2%} | "
                       f"{metrics['coverage']:.1f}% |\n")

        # Mejor m√©todo
        best_method = max(
            self.evaluation_results.keys(),
            key=lambda m: self.evaluation_results[m]['metrics']['f1_score']
        )
        best_f1 = self.evaluation_results[best_method]['metrics']['f1_score']

        content += f"\n**Mejor m√©todo**: {best_method} (F1-Score: {best_f1:.2%})\n"

        content += "\n---\n\n## Visualizaciones\n\n"
        content += "### Comparaci√≥n de M√©tricas\n"
        content += "![Comparaci√≥n de M√©tricas](../visualizations/metrics_comparison.png)\n\n"

        content += "### Distribuci√≥n de Frecuencias\n"
        content += "![Distribuci√≥n de Frecuencias](../visualizations/frequency_distribution.png)\n\n"

        content += "### Overlap entre M√©todos\n"
        content += "![Overlap entre M√©todos](../visualizations/methods_overlap.png)\n\n"

        content += "### Top T√©rminos por M√©todo\n"
        content += "![Top T√©rminos](../visualizations/top_terms_comparison.png)\n\n"

        content += "---\n\n## Parte 1: An√°lisis de T√©rminos Predefinidos\n\n"

        # Top t√©rminos
        content += "### Top 10 T√©rminos M√°s Frecuentes\n\n"
        content += "| Posici√≥n | T√©rmino | Frecuencia |\n"
        content += "|----------|---------|------------|\n"

        top_10 = sorted(
            self.predefined_results['frequencies'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:10]

        for i, (term, freq) in enumerate(top_10, 1):
            content += f"| {i} | {term} | {freq} |\n"

        content += f"\nüìÑ [Ver reporte completo](predefined_terms_report.md)\n"

        content += "\n---\n\n## Parte 2: Extracci√≥n Autom√°tica\n\n"

        # T√©rminos extra√≠dos por m√©todo
        for method in ['RAKE', 'TextRank', 'Combined']:
            content += f"### M√©todo: {method}\n\n"

            if method == 'RAKE':
                terms = self.extracted_results['rake_terms'][:5]
            elif method == 'TextRank':
                terms = self.extracted_results['textrank_terms'][:5]
            else:
                terms = self.extracted_results['combined_terms'][:5]

            content += "**Top 5 t√©rminos**:\n"
            for i, term in enumerate(terms, 1):
                content += f"{i}. {term}\n"
            content += "\n"

        content += f"üìÑ [Ver reporte completo de extracci√≥n](extracted_terms_report.md)\n"

        content += "\n---\n\n## Parte 3: Evaluaci√≥n de Precisi√≥n\n\n"

        # Detalles por m√©todo
        for method in ['RAKE', 'TextRank', 'Combined']:
            metrics = self.evaluation_results[method]['metrics']
            matches = self.evaluation_results[method]['matches']

            content += f"### Evaluaci√≥n: {method}\n\n"
            content += "| M√©trica | Valor |\n"
            content += "|---------|-------|\n"
            content += f"| Precision | {metrics['precision']:.2%} |\n"
            content += f"| Recall | {metrics['recall']:.2%} |\n"
            content += f"| F1-Score | {metrics['f1_score']:.2%} |\n"
            content += f"| Exact Matches | {metrics['n_exact_matches']} |\n"
            content += f"| Partial Matches | {metrics['n_partial_matches']} |\n"
            content += f"| Novel Terms | {metrics['n_novel_terms']} |\n"
            content += "\n"

            content += f"üìÑ [Ver reporte completo de evaluaci√≥n](evaluation_{method.lower()}.md)\n\n"

        content += "---\n\n## Conclusiones y Recomendaciones\n\n"

        content += self._generate_conclusions()

        content += "\n---\n\n## Archivos Generados\n\n"
        content += "### Datos\n"
        content += "- `data/predefined_terms_frequencies.csv`: Frecuencias de t√©rminos predefinidos\n"
        content += "- `data/extracted_terms_all_methods.csv`: T√©rminos extra√≠dos por todos los m√©todos\n"
        content += "- `data/evaluation_metrics.json`: M√©tricas de evaluaci√≥n en formato JSON\n\n"

        content += "### Reportes\n"
        content += "- `reports/predefined_terms_report.md`: An√°lisis detallado de t√©rminos predefinidos\n"
        content += "- `reports/extracted_terms_report.md`: Reporte de extracci√≥n autom√°tica\n"
        content += "- `reports/evaluation_rake.md`: Evaluaci√≥n del m√©todo RAKE\n"
        content += "- `reports/evaluation_textrank.md`: Evaluaci√≥n del m√©todo TextRank\n"
        content += "- `reports/evaluation_combined.md`: Evaluaci√≥n del m√©todo combinado\n\n"

        content += "### Visualizaciones\n"
        content += "- `visualizations/metrics_comparison.png`: Comparaci√≥n de m√©tricas entre m√©todos\n"
        content += "- `visualizations/frequency_distribution.png`: Distribuci√≥n de frecuencias\n"
        content += "- `visualizations/methods_overlap.png`: Diagrama de Venn de overlap\n"
        content += "- `visualizations/top_terms_comparison.png`: Comparaci√≥n de top t√©rminos\n"

        content += "\n---\n\n*Reporte generado autom√°ticamente por TermAnalysisPipeline*\n"

        return content

    def _generate_conclusions(self) -> str:
        """Genera conclusiones autom√°ticas basadas en resultados."""

        conclusions = "### Hallazgos Principales\n\n"

        # Mejor m√©todo
        best_method = max(
            self.evaluation_results.keys(),
            key=lambda m: self.evaluation_results[m]['metrics']['f1_score']
        )
        best_metrics = self.evaluation_results[best_method]['metrics']

        conclusions += f"1. **Mejor M√©todo**: {best_method} demostr√≥ el mejor desempe√±o general "
        conclusions += f"con F1-Score de {best_metrics['f1_score']:.2%}.\n\n"

        # An√°lisis de precision/recall
        for method in self.evaluation_results.keys():
            metrics = self.evaluation_results[method]['metrics']

            if metrics['precision'] > 0.7 and metrics['recall'] > 0.7:
                conclusions += f"2. **{method}**: Balance excelente entre precision y recall, "
                conclusions += "adecuado para uso en producci√≥n.\n\n"
            elif metrics['precision'] > metrics['recall']:
                conclusions += f"2. **{method}**: Alta precision ({metrics['precision']:.2%}) "
                conclusions += "pero recall moderado, extrae pocos t√©rminos pero de alta calidad.\n\n"
            elif metrics['recall'] > metrics['precision']:
                conclusions += f"2. **{method}**: Alto recall ({metrics['recall']:.2%}) "
                conclusions += "pero precision moderada, captura muchos conceptos con algo de ruido.\n\n"

        # Novel terms
        total_novel = sum(
            self.evaluation_results[m]['metrics']['n_novel_terms']
            for m in self.evaluation_results.keys()
        )

        if total_novel > 20:
            conclusions += f"3. **Descubrimiento**: Se identificaron {total_novel} t√©rminos nuevos "
            conclusions += "que no estaban en los t√©rminos predefinidos, sugiriendo conceptos emergentes.\n\n"

        conclusions += "\n### Recomendaciones\n\n"

        if best_metrics['f1_score'] >= 0.7:
            conclusions += f"- Usar **{best_method}** como m√©todo principal de extracci√≥n.\n"
        else:
            conclusions += "- Considerar ajustar par√°metros o combinar m√∫ltiples m√©todos.\n"

        conclusions += "- Revisar t√©rminos nuevos de alta frecuencia para actualizar t√©rminos predefinidos.\n"
        conclusions += "- Analizar t√©rminos predefinidos no encontrados para entender por qu√© no aparecen.\n"

        return conclusions


def run_complete_analysis(unified_data_path: str, output_dir: str):
    """
    Ejecuta an√°lisis completo de t√©rminos.

    Args:
        unified_data_path: Ruta al archivo unified_abstracts.json
        output_dir: Directorio para guardar outputs

    Returns:
        Instancia de TermAnalysisPipeline con todos los resultados
    """
    print("\n" + "="*70)
    print(" PIPELINE COMPLETO DE AN√ÅLISIS DE T√âRMINOS")
    print("="*70)
    print(f"Input:  {unified_data_path}")
    print(f"Output: {output_dir}")
    print("="*70)

    # Crear pipeline
    pipeline = TermAnalysisPipeline(unified_data_path, output_dir)

    # 1. Cargar datos
    pipeline.load_data()

    # 2. Analizar t√©rminos predefinidos
    pipeline.analyze_predefined_terms()

    # 3. Extraer t√©rminos autom√°ticamente
    pipeline.extract_terms_automatically()

    # 4. Evaluar precisi√≥n
    pipeline.evaluate_precision()

    # 5. Crear visualizaciones comparativas
    pipeline.create_comparative_visualizations()

    # 6. Generar reporte maestro
    pipeline.generate_master_report()

    print("\n" + "="*70)
    print(" AN√ÅLISIS COMPLETO FINALIZADO")
    print("="*70)
    print(f"\n‚úì Todos los resultados guardados en: {output_dir}")
    print(f"\nüìä Archivos generados:")
    print(f"  ‚Ä¢ Reporte maestro: {output_dir}/reports/term_analysis_report.md")
    print(f"  ‚Ä¢ Datos CSV: {output_dir}/data/")
    print(f"  ‚Ä¢ Visualizaciones: {output_dir}/visualizations/")
    print(f"  ‚Ä¢ Reportes detallados: {output_dir}/reports/")

    return pipeline


def main():
    """Ejemplo de uso del pipeline."""

    # Configuraci√≥n
    unified_data_path = 'unified_abstracts.json'
    output_dir = 'term_analysis_output'

    # Verificar que existe el archivo
    if not os.path.exists(unified_data_path):
        print(f"\n‚ùå Error: No se encontr√≥ el archivo {unified_data_path}")
        print("\nAseg√∫rate de:")
        print("  1. Haber ejecutado el buscador acad√©mico")
        print("  2. Tener el archivo unified_abstracts.json en el directorio actual")
        return

    # Ejecutar an√°lisis completo
    pipeline = run_complete_analysis(unified_data_path, output_dir)

    print("\n‚úì Pipeline ejecutado exitosamente!")


if __name__ == "__main__":
    main()

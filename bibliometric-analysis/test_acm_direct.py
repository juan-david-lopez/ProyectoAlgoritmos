"""
Prueba directa del scraper ACM actualizado
Usa la configuraciÃ³n existente del proyecto
"""

import sys
from pathlib import Path

# Configurar path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from loguru import logger
from src.utils.config_loader import get_config
from src.scrapers.acm_scraper import ACMScraper

# Configurar logger
logger.remove()
logger.add(sys.stderr, level="INFO", format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")
logger.add("logs/test_acm.log", rotation="10 MB")

def main():
    """Prueba completa del scraper ACM"""
    
    print("\n" + "="*80)
    print("ğŸ§ª PRUEBA DEL SCRAPER ACM - VERSIÃ“N ACTUALIZADA")
    print("="*80 + "\n")
    
    try:
        # Cargar configuraciÃ³n
        logger.info("Cargando configuraciÃ³n del proyecto...")
        config = get_config()
        logger.success("ConfiguraciÃ³n cargada correctamente")
        
        # Crear scraper
        logger.info("Inicializando scraper ACM (navegador visible)...")
        scraper = ACMScraper(config, headless=False)
        logger.success("Scraper inicializado")
        
        # Prueba 1: BÃºsqueda pequeÃ±a
        print("\n" + "-"*80)
        print("ğŸ“‹ PRUEBA 1: BÃºsqueda y extracciÃ³n de artÃ­culos")
        print("-"*80)
        
        query = "artificial intelligence"
        max_results = 3
        
        logger.info(f"Buscando: '{query}' (mÃ¡ximo {max_results} resultados)")
        
        articles = scraper.search(query, max_results=max_results)
        
        if not articles:
            logger.error("âŒ No se encontraron artÃ­culos")
            scraper.close()
            return False
        
        logger.success(f"âœ… Encontrados {len(articles)} artÃ­culos")
        
        # Mostrar detalles de los artÃ­culos
        print("\nğŸ“š ArtÃ­culos encontrados:")
        for i, article in enumerate(articles, 1):
            print(f"\n  {i}. {article.get('title', 'Sin tÃ­tulo')[:70]}...")
            print(f"     Autores: {article.get('authors', 'N/A')}")
            print(f"     AÃ±o: {article.get('year', 'N/A')}")
            print(f"     DOI: {article.get('doi', 'N/A')}")
            print(f"     URL: {article.get('url', 'N/A')[:60]}...")
        
        # Prueba 2: GeneraciÃ³n de archivo BibTeX
        print("\n" + "-"*80)
        print("ğŸ“ PRUEBA 2: GeneraciÃ³n de archivo BibTeX")
        print("-"*80)
        
        logger.info("Generando archivo BibTeX...")
        bibtex_file = scraper.download_results(format='bibtex')
        
        if not bibtex_file or not bibtex_file.exists():
            logger.error("âŒ Error al generar archivo BibTeX")
            scraper.close()
            return False
        
        logger.success(f"âœ… Archivo BibTeX generado: {bibtex_file.name}")
        print(f"\n   ğŸ“ Ruta: {bibtex_file}")
        print(f"   ğŸ“Š TamaÃ±o: {bibtex_file.stat().st_size:,} bytes")
        
        # Mostrar contenido del archivo
        content = bibtex_file.read_text(encoding='utf-8')
        lines = content.split('\n')
        
        print(f"\n   ğŸ“– Primeras 20 lÃ­neas del archivo BibTeX:")
        print("   " + "-"*70)
        for line in lines[:20]:
            print(f"   {line}")
        print("   " + "-"*70)
        
        # Prueba 3: GeneraciÃ³n de archivo JSON
        print("\n" + "-"*80)
        print("ğŸ“ PRUEBA 3: GeneraciÃ³n de archivo JSON")
        print("-"*80)
        
        logger.info("Generando archivo JSON...")
        json_file = scraper.download_results(format='json')
        
        if not json_file or not json_file.exists():
            logger.error("âŒ Error al generar archivo JSON")
            scraper.close()
            return False
        
        logger.success(f"âœ… Archivo JSON generado: {json_file.name}")
        print(f"\n   ğŸ“ Ruta: {json_file}")
        print(f"   ğŸ“Š TamaÃ±o: {json_file.stat().st_size:,} bytes")
        
        # Mostrar primeras lÃ­neas
        json_content = json_file.read_text(encoding='utf-8')
        json_lines = json_content.split('\n')
        
        print(f"\n   ğŸ“– Primeras 15 lÃ­neas del archivo JSON:")
        print("   " + "-"*70)
        for line in json_lines[:15]:
            print(f"   {line}")
        print("   " + "-"*70)
        
        # Prueba 4: GeneraciÃ³n de archivo CSV
        print("\n" + "-"*80)
        print("ğŸ“ PRUEBA 4: GeneraciÃ³n de archivo CSV")
        print("-"*80)
        
        logger.info("Generando archivo CSV...")
        csv_file = scraper.download_results(format='csv')
        
        if not csv_file or not csv_file.exists():
            logger.error("âŒ Error al generar archivo CSV")
            scraper.close()
            return False
        
        logger.success(f"âœ… Archivo CSV generado: {csv_file.name}")
        print(f"\n   ğŸ“ Ruta: {csv_file}")
        print(f"   ğŸ“Š TamaÃ±o: {csv_file.stat().st_size:,} bytes")
        
        # Mostrar contenido
        csv_content = csv_file.read_text(encoding='utf-8')
        csv_lines = csv_content.split('\n')
        
        print(f"\n   ğŸ“– Contenido del archivo CSV:")
        print("   " + "-"*70)
        for line in csv_lines[:5]:
            print(f"   {line[:75]}...")
        print("   " + "-"*70)
        
        # Prueba 5: Parseo de archivos
        print("\n" + "-"*80)
        print("ğŸ“š PRUEBA 5: Parseo de archivo BibTeX")
        print("-"*80)
        
        logger.info("Parseando archivo BibTeX...")
        records = scraper.parse_file(bibtex_file)
        
        if not records:
            logger.error("âŒ Error al parsear archivo")
            scraper.close()
            return False
        
        logger.success(f"âœ… Parseados {len(records)} registros")
        
        # Mostrar primer registro parseado
        if records:
            first_record = records[0]
            print("\n   ğŸ“„ Primer registro parseado:")
            print(f"      ID: {first_record.get('id', 'N/A')}")
            print(f"      TÃ­tulo: {first_record.get('title', 'N/A')[:60]}...")
            print(f"      Autores: {first_record.get('authors', [])}")
            print(f"      AÃ±o: {first_record.get('year', 'N/A')}")
            print(f"      DOI: {first_record.get('doi', 'N/A')}")
            print(f"      Fuente: {first_record.get('source', 'N/A')}")
            print(f"      Publisher: {first_record.get('publisher', 'N/A')}")
        
        # Cerrar navegador
        logger.info("ğŸ§¹ Cerrando navegador...")
        scraper.close()
        logger.success("âœ… Navegador cerrado")
        
        # Resumen final
        print("\n" + "="*80)
        print("ğŸ‰ TODAS LAS PRUEBAS COMPLETADAS EXITOSAMENTE")
        print("="*80)
        
        print("\nğŸ“Š RESUMEN:")
        print(f"   âœ… BÃºsqueda: {len(articles)} artÃ­culos encontrados")
        print(f"   âœ… BibTeX: {bibtex_file.name}")
        print(f"   âœ… JSON: {json_file.name}")
        print(f"   âœ… CSV: {csv_file.name}")
        print(f"   âœ… Parseo: {len(records)} registros")
        
        print("\nğŸ¯ El scraper ACM estÃ¡ completamente funcional!")
        print("   - ExtracciÃ³n directa de HTML âœ…")
        print("   - PaginaciÃ³n automÃ¡tica âœ…")
        print("   - MÃºltiples formatos de salida âœ…")
        print("   - Parseo de archivos âœ…")
        
        print("\n" + "="*80 + "\n")
        
        return True
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ Prueba interrumpida por el usuario")
        return False
        
    except Exception as e:
        logger.error(f"\nâŒ Error en la prueba: {e}")
        logger.exception("Traceback completo:")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

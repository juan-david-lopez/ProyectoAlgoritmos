# ğŸ“Š Bibliometric Analysis - Inteligencia Artificial Generativa

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## ğŸ“– DescripciÃ³n del Proyecto

Sistema completo de **anÃ¡lisis bibliomÃ©trico** diseÃ±ado para el estudio sistemÃ¡tico de publicaciones cientÃ­ficas sobre **"inteligencia artificial generativa"**.

El proyecto implementa un pipeline end-to-end que incluye:

- ğŸ” **Descarga automatizada** de datos desde bases acadÃ©micas (IEEE Xplore, Scopus, Web of Science)
- ğŸ”„ **DetecciÃ³n inteligente de duplicados** usando mÃºltiples algoritmos de similitud
- ğŸ§¹ **Preprocesamiento avanzado** con NLP en espaÃ±ol e inglÃ©s
- ğŸ“ˆ **Clustering temÃ¡tico** con K-means, DBSCAN y clustering jerÃ¡rquico
- ğŸ“Š **VisualizaciÃ³n interactiva** de tendencias, redes y distribuciones
- ğŸ“„ **GeneraciÃ³n automÃ¡tica de reportes** en formato PDF con estadÃ­sticas y grÃ¡ficos

### ğŸ¯ Objetivos

1. Automatizar la recolecciÃ³n de datos bibliogrÃ¡ficos de mÃºltiples fuentes
2. Identificar y eliminar publicaciones duplicadas con alta precisiÃ³n
3. Analizar tendencias temporales y geogrÃ¡ficas en investigaciÃ³n de IA generativa
4. Descubrir agrupaciones temÃ¡ticas mediante tÃ©cnicas de machine learning
5. Generar visualizaciones y reportes profesionales para anÃ¡lisis acadÃ©mico

## ğŸ“ Estructura del Proyecto

```
bibliometric-analysis/
â”œâ”€â”€ ğŸ“‚ config/                  # ConfiguraciÃ³n
â”‚   â”œâ”€â”€ config.yaml            # ParÃ¡metros principales (640+ lÃ­neas)
â”‚   â””â”€â”€ .env.example           # Template de variables de entorno
â”‚
â”œâ”€â”€ ğŸ“‚ data/                    # Datos (no versionados)
â”‚   â”œâ”€â”€ raw/                   # Datos descargados originales
â”‚   â”œâ”€â”€ processed/             # Datos procesados y limpios
â”‚   â””â”€â”€ duplicates/            # Registros duplicados identificados
â”‚
â”œâ”€â”€ ğŸ“‚ src/                     # CÃ³digo fuente
â”‚   â”œâ”€â”€ scrapers/              # MÃ³dulos de descarga
â”‚   â”‚   â”œâ”€â”€ ieee_scraper.py
â”‚   â”‚   â”œâ”€â”€ scopus_scraper.py
â”‚   â”‚   â””â”€â”€ wos_scraper.py
â”‚   â”‚
â”‚   â”œâ”€â”€ algorithms/            # Algoritmos de similitud
â”‚   â”‚   â”œâ”€â”€ levenshtein.py
â”‚   â”‚   â”œâ”€â”€ jaro_winkler.py
â”‚   â”‚   â””â”€â”€ jaccard.py
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/         # Limpieza y preprocesamiento
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py
â”‚   â”‚   â”œâ”€â”€ deduplicator.py
â”‚   â”‚   â””â”€â”€ text_processor.py
â”‚   â”‚
â”‚   â”œâ”€â”€ clustering/            # Algoritmos de clustering
â”‚   â”‚   â”œâ”€â”€ kmeans_clustering.py
â”‚   â”‚   â”œâ”€â”€ dbscan_clustering.py
â”‚   â”‚   â””â”€â”€ hierarchical_clustering.py
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/         # GeneraciÃ³n de grÃ¡ficos
â”‚   â”‚   â”œâ”€â”€ temporal_plots.py
â”‚   â”‚   â”œâ”€â”€ geographic_maps.py
â”‚   â”‚   â”œâ”€â”€ network_graphs.py
â”‚   â”‚   â””â”€â”€ cluster_plots.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                 # Utilidades generales
â”‚       â”œâ”€â”€ config_loader.py   # Carga de configuraciÃ³n
â”‚       â”œâ”€â”€ logger.py          # Sistema de logging
â”‚       â””â”€â”€ file_handler.py    # Manejo de archivos
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                 # Scripts de utilidad
â”‚   â”œâ”€â”€ verify_installation.py
â”‚   â””â”€â”€ download_nlp_models.py
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                    # DocumentaciÃ³n
â”‚   â””â”€â”€ SETUP.md               # GuÃ­a de instalaciÃ³n detallada
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                   # Tests unitarios
â”‚   â”œâ”€â”€ test_scrapers.py
â”‚   â”œâ”€â”€ test_algorithms.py
â”‚   â””â”€â”€ test_clustering.py
â”‚
â”œâ”€â”€ ğŸ“‚ outputs/                 # Resultados (no versionados)
â”‚   â”œâ”€â”€ reports/               # Reportes PDF generados
â”‚   â””â”€â”€ visualizations/        # GrÃ¡ficos e imÃ¡genes
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/               # Jupyter notebooks
â”‚   â”œâ”€â”€ exploratory_analysis.ipynb
â”‚   â””â”€â”€ results_visualization.ipynb
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                    # Logs de ejecuciÃ³n
â”‚
â”œâ”€â”€ main.py                    # Punto de entrada principal
â”œâ”€â”€ requirements.txt           # Dependencias Python (50+ paquetes)
â”œâ”€â”€ README.md                  # Este archivo
â””â”€â”€ .gitignore                 # Archivos excluidos de git
```

## ğŸ¯ Requerimientos del Proyecto

### 1ï¸âƒ£ Descarga de Datos (Web Scraping)

**DescripciÃ³n**: AutomatizaciÃ³n de la descarga de publicaciones cientÃ­ficas desde mÃºltiples bases de datos acadÃ©micas.

**Fuentes de datos**:
- ğŸ“š **IEEE Xplore**: Publicaciones de ingenierÃ­a y tecnologÃ­a
- ğŸ“š **Scopus**: Base de datos multidisciplinaria de Elsevier
- ğŸ“š **Web of Science**: Ãndice de citas de Clarivate

**CaracterÃ­sticas**:
- Query: "inteligencia artificial generativa"
- Soporte para API y web scraping
- Rate limiting automÃ¡tico
- Manejo de errores y reintentos
- ExtracciÃ³n de campos: tÃ­tulo, autores, abstract, DOI, aÃ±o, keywords, citas

**Formato de salida**: CSV con campos estandarizados

---

### 2ï¸âƒ£ DetecciÃ³n de Duplicados

**DescripciÃ³n**: IdentificaciÃ³n y eliminaciÃ³n de publicaciones duplicadas usando mÃºltiples algoritmos de similitud de texto.

**Algoritmos implementados**:
- ğŸ”¤ **Levenshtein Distance**: Distancia de ediciÃ³n entre cadenas
- ğŸ”¤ **Jaro-Winkler**: Similitud de cadenas con Ã©nfasis en prefijos
- ğŸ”¤ **Jaccard Index**: Similitud basada en conjuntos de palabras

**Campos analizados**:
- TÃ­tulo (weight: 0.4)
- Abstract (weight: 0.3)
- DOI (exact match)
- Autores (weight: 0.3)

**Thresholds**:
- Similitud de tÃ­tulo: â‰¥ 85%
- Similitud de abstract: â‰¥ 80%
- Similitud combinada: â‰¥ 75%

**Salida**: Archivo de duplicados con ID de grupo y mÃ©tricas de similitud

---

### 3ï¸âƒ£ Preprocesamiento de Datos

**DescripciÃ³n**: Limpieza, normalizaciÃ³n y transformaciÃ³n de datos bibliogrÃ¡ficos.

**Operaciones**:
- âœ… NormalizaciÃ³n de texto (lowercase, whitespace)
- âœ… EliminaciÃ³n de HTML tags, URLs, emails
- âœ… TokenizaciÃ³n y lemmatizaciÃ³n (spaCy)
- âœ… EliminaciÃ³n de stop words (espaÃ±ol/inglÃ©s)
- âœ… ValidaciÃ³n de campos requeridos
- âœ… EstandarizaciÃ³n de formatos de fecha
- âœ… Parsing de listas de autores

**Lenguajes soportados**: EspaÃ±ol (primario), InglÃ©s (secundario)

---

### 4ï¸âƒ£ Clustering TemÃ¡tico

**DescripciÃ³n**: AgrupaciÃ³n automÃ¡tica de publicaciones por similitud temÃ¡tica usando tÃ©cnicas de machine learning.

**Algoritmos**:

1. **K-Means**
   - NÃºmero de clusters: 5 (configurable)
   - Feature extraction: TF-IDF o Sentence Transformers
   - OptimizaciÃ³n automÃ¡tica con mÃ©todo del codo

2. **DBSCAN**
   - Epsilon: 0.5 (auto-tuning disponible)
   - Min samples: 5
   - DetecciÃ³n automÃ¡tica de outliers

3. **Clustering JerÃ¡rquico**
   - Linkage: Ward
   - GeneraciÃ³n de dendrogramas
   - Corte adaptativo

**Features utilizadas**:
- TÃ­tulo + Abstract + Keywords
- VectorizaciÃ³n con TF-IDF (1000 features)
- ReducciÃ³n dimensional: PCA/t-SNE/UMAP

**EvaluaciÃ³n**:
- Silhouette Score
- Calinski-Harabasz Score
- Davies-Bouldin Score

---

### 5ï¸âƒ£ VisualizaciÃ³n de Resultados

**DescripciÃ³n**: GeneraciÃ³n de grÃ¡ficos interactivos y estÃ¡ticos para anÃ¡lisis visual de resultados.

**Visualizaciones implementadas**:

ğŸ“ˆ **Temporal**:
- Tendencia de publicaciones por aÃ±o
- Tasa de crecimiento

ğŸŒ **GeogrÃ¡fica**:
- Mapa coroplÃ©tico de distribuciÃ³n por paÃ­s
- Top 20 paÃ­ses productores

ğŸ“Š **DistribuciÃ³n**:
- Publicaciones por fuente (IEEE, Scopus, WOS)
- Top journals y conferencias

ğŸ•¸ï¸ **Redes**:
- Red de coautorÃ­a (NetworkX)
- Comunidades de investigaciÃ³n

â˜ï¸ **AnÃ¡lisis de texto**:
- Word cloud de keywords
- Frecuencia de tÃ©rminos

ğŸ“ **Clustering**:
- Scatter plot 2D/3D de clusters
- Heatmap de similitud

**Formatos de salida**: PNG (300 DPI), SVG, HTML interactivo (Plotly)

---

### 6ï¸âƒ£ Reporte Automatizado

**DescripciÃ³n**: GeneraciÃ³n automÃ¡tica de reportes profesionales en formato PDF con anÃ¡lisis completo.

**Secciones del reporte**:
1. ğŸ“„ Portada con metadata
2. ğŸ“‘ Tabla de contenidos
3. ğŸ“ Resumen ejecutivo
4. ğŸ”¬ MetodologÃ­a
5. ğŸ“Š Resultados y estadÃ­sticas
6. ğŸ“ˆ Visualizaciones
7. ğŸ’¡ Conclusiones
8. ğŸ“š Referencias

**EstadÃ­sticas incluidas**:
- Total de publicaciones
- DistribuciÃ³n temporal
- Top autores (mÃ¡s productivos, mÃ¡s citados)
- Top paÃ­ses e instituciones
- Top fuentes de publicaciÃ³n
- MÃ©tricas de citas
- AnÃ¡lisis de keywords
- Resumen de clusters

**Formato**: PDF con tipografÃ­a profesional (Times New Roman + Arial)

## âš™ï¸ InstalaciÃ³n

### ğŸš€ InstalaciÃ³n RÃ¡pida

```bash
# 1. Navegar al directorio del proyecto
cd bibliometric-analysis

# 2. Crear entorno virtual
python -m venv venv

# 3. Activar entorno virtual
# En Windows:
venv\Scripts\activate
# En macOS/Linux:
source venv/bin/activate

# 4. Actualizar pip
python -m pip install --upgrade pip

# 5. Instalar dependencias (10-15 minutos)
pip install -r requirements.txt

# 6. Descargar modelos NLP
python scripts/download_nlp_models.py

# 7. Verificar instalaciÃ³n
python scripts/verify_installation.py

# 8. Configurar variables de entorno
cp config/.env.example config/.env
# Editar config/.env con tus credenciales de API (opcional)
```

### ğŸ“‹ Requisitos del Sistema

- **Python**: 3.8 o superior
- **RAM**: MÃ­nimo 8 GB (recomendado 16 GB)
- **Espacio**: 10 GB libres
- **SO**: Windows 10+, macOS 10.14+, Linux (Ubuntu 18.04+)

### ğŸ”‘ ConfiguraciÃ³n de API Keys (Opcional)

Para usar las APIs oficiales (en lugar de web scraping):

1. **Scopus API**: Registrarse en https://dev.elsevier.com/
2. **Web of Science API**: Registrarse en https://developer.clarivate.com/
3. **IEEE Xplore API**: Registrarse en https://developer.ieee.org/

Agregar las keys en `config/.env`:
```bash
SCOPUS_API_KEY=tu_clave_aqui
WOS_API_KEY=tu_clave_aqui
IEEE_API_KEY=tu_clave_aqui
```

### ğŸ“– InstalaciÃ³n Detallada

Ver [docs/SETUP.md](docs/SETUP.md) para:
- Instrucciones paso a paso
- Troubleshooting
- ConfiguraciÃ³n avanzada
- InstalaciÃ³n de wkhtmltopdf para PDFs

---

## ğŸš€ CÃ³mo Usar

### EjecuciÃ³n Completa (Pipeline End-to-End)

```bash
# Ejecutar todos los requerimientos en secuencia
python main.py --mode full
```

### EjecuciÃ³n por MÃ³dulos

#### 1ï¸âƒ£ Descarga de Datos

```bash
# Ejecutar todos los scrapers
python main.py --mode scrape

# O ejecutar scrapers individuales
python -m src.scrapers.ieee_scraper
python -m src.scrapers.scopus_scraper
python -m src.scrapers.wos_scraper
```

**Salida**: `data/raw/publications_{source}_{timestamp}.csv`

---

#### 2ï¸âƒ£ DetecciÃ³n y EliminaciÃ³n de Duplicados

```bash
# Ejecutar deduplicaciÃ³n
python main.py --mode preprocess

# O ejecutar directamente
python -m src.preprocessing.deduplicator \
    --input data/raw/ \
    --output data/processed/publications_clean.csv \
    --duplicates data/duplicates/duplicates.csv
```

**ParÃ¡metros configurables** (en `config/config.yaml`):
- Thresholds de similitud
- Algoritmos a usar
- Campos a comparar

**Salida**:
- `data/processed/publications_clean.csv` - Datos sin duplicados
- `data/duplicates/duplicates.csv` - Registros duplicados con mÃ©tricas

---

#### 3ï¸âƒ£ Preprocesamiento de Texto

```bash
# Preprocesamiento automÃ¡tico
python -m src.preprocessing.text_processor \
    --input data/processed/publications_clean.csv \
    --output data/processed/publications_preprocessed.csv
```

**Operaciones realizadas**:
- Limpieza de texto
- TokenizaciÃ³n
- LemmatizaciÃ³n (spaCy)
- EliminaciÃ³n de stop words
- ValidaciÃ³n de datos

---

#### 4ï¸âƒ£ Clustering TemÃ¡tico

```bash
# Ejecutar todos los algoritmos
python main.py --mode cluster

# O ejecutar algoritmos individuales
python -m src.clustering.kmeans_clustering
python -m src.clustering.dbscan_clustering
python -m src.clustering.hierarchical_clustering
```

**ConfiguraciÃ³n** (`config/config.yaml`):
```yaml
clustering:
  algorithms:
    kmeans:
      n_clusters: 5
    dbscan:
      eps: 0.5
      min_samples: 5
```

**Salida**:
- `data/processed/publications_clustered.csv` - Datos con labels de cluster
- `outputs/clustering_metrics.json` - MÃ©tricas de evaluaciÃ³n

---

#### 5ï¸âƒ£ VisualizaciÃ³n

```bash
# Generar todas las visualizaciones
python main.py --mode visualize

# O generar visualizaciones especÃ­ficas
python -m src.visualization.temporal_plots
python -m src.visualization.geographic_maps
python -m src.visualization.network_graphs
python -m src.visualization.cluster_plots
```

**Salida**: `outputs/visualizations/`
- `temporal_trends.png`
- `country_distribution.png`
- `coauthorship_network.png`
- `keyword_cloud.png`
- `cluster_visualization.png`
- `*.html` - Versiones interactivas (Plotly)

---

#### 6ï¸âƒ£ GeneraciÃ³n de Reporte

```bash
# Generar reporte completo en PDF
python main.py --mode report

# O ejecutar directamente
python -m src.visualization.report_generator \
    --output outputs/reports/bibliometric_analysis_report.pdf
```

**Salida**: `outputs/reports/bibliometric_analysis_report_{timestamp}.pdf`

---

### Ejemplos de Uso Avanzado

```bash
# Ejecutar solo IEEE y Scopus
python main.py --mode scrape --sources ieee,scopus

# Clustering con K-Means solamente
python -m src.clustering.kmeans_clustering --n-clusters 7

# Generar solo visualizaciones temporales
python -m src.visualization.temporal_plots --years 2018-2024

# Reporte en HTML en lugar de PDF
python main.py --mode report --format html
```

### Interfaz Web Interactiva (Streamlit)

```bash
# Lanzar dashboard interactivo
streamlit run app.py

# Acceder en el navegador
# http://localhost:8501
```

### Jupyter Notebooks

```bash
# Iniciar Jupyter
jupyter notebook

# Abrir notebooks en notebooks/
# - exploratory_analysis.ipynb
# - results_visualization.ipynb
```

---

## ğŸ› ï¸ TecnologÃ­as y Herramientas

### Core Technologies
- ![Python](https://img.shields.io/badge/Python-3.8+-3776AB?logo=python&logoColor=white) **Python 3.8+**
- **Data Processing**: Pandas, NumPy, OpenPyXL
- **Web Scraping**: Selenium, BeautifulSoup4, Requests, WebDriver Manager

### NLP & Machine Learning
- **NLP**: NLTK, spaCy, python-Levenshtein, Jellyfish
- **ML/Clustering**: Scikit-learn, SciPy
- **Deep Learning**: PyTorch (CPU), Transformers, Sentence-Transformers

### Visualization & Reporting
- **Charts**: Matplotlib, Seaborn, Plotly, WordCloud
- **Maps**: Folium (mapas interactivos)
- **Networks**: NetworkX
- **Reports**: ReportLab, FPDF, PDFKit

### Additional Tools
- **Web App**: Streamlit
- **Bibliographic Parsing**: python-RISparser, bibtexparser
- **Utilities**: PyYAML, python-dotenv, tqdm, loguru

**Total**: ~50 paquetes de Python

---

## ğŸ“Š ConfiguraciÃ³n

Todo el proyecto es configurable mediante archivos:

### `config/config.yaml` (640+ lÃ­neas)
ConfiguraciÃ³n completa de:
- Queries y fuentes de datos
- ParÃ¡metros de scraping y rate limiting
- Thresholds de deduplicaciÃ³n
- ConfiguraciÃ³n de clustering (K-means, DBSCAN, Hierarchical)
- Estilos de visualizaciÃ³n
- Formato de reportes

### `config/.env`
Variables de entorno sensibles:
- API keys (Scopus, WOS, IEEE)
- Credenciales de base de datos
- ConfiguraciÃ³n de scraping

### Ejemplo de configuraciÃ³n:

```yaml
# config/config.yaml
query:
  keywords: "inteligencia artificial generativa"
  date_range:
    start: "2018-01-01"
    end: null

clustering:
  algorithms:
    kmeans:
      n_clusters: 5
    dbscan:
      eps: 0.5
```

---

## ğŸ“ˆ Estado del Proyecto

### Infraestructura âœ…
- [x] Estructura de carpetas completa
- [x] ConfiguraciÃ³n exhaustiva (YAML + .env)
- [x] Sistema de logging
- [x] Utilidad de carga de configuraciÃ³n
- [x] Scripts de instalaciÃ³n y verificaciÃ³n
- [x] .gitignore completo (330+ lÃ­neas)

### Requerimientos del Proyecto
- [ ] 1ï¸âƒ£ MÃ³dulos de descarga (IEEE, Scopus, WOS)
- [ ] 2ï¸âƒ£ Sistema de deduplicaciÃ³n (Levenshtein, Jaro-Winkler, Jaccard)
- [ ] 3ï¸âƒ£ Preprocesamiento de datos
- [ ] 4ï¸âƒ£ Clustering (K-means, DBSCAN, JerÃ¡rquico)
- [ ] 5ï¸âƒ£ VisualizaciÃ³n (9 tipos de grÃ¡ficos)
- [ ] 6ï¸âƒ£ GeneraciÃ³n de reportes PDF

### PrÃ³ximos Pasos
1. Implementar scrapers para las 3 fuentes de datos
2. Desarrollar sistema de deduplicaciÃ³n
3. Crear pipeline de preprocesamiento
4. Implementar algoritmos de clustering
5. Generar visualizaciones
6. Crear generador de reportes

---

## ğŸ¤ Contribuir

### Estructura de Commits
```bash
git commit -m "feat: Add IEEE scraper implementation"
git commit -m "fix: Resolve duplicate detection threshold issue"
git commit -m "docs: Update README with usage examples"
```

### Testing
```bash
# Ejecutar tests
pytest tests/

# Con coverage
pytest --cov=src tests/

# Test especÃ­fico
pytest tests/test_scrapers.py::test_ieee_scraper
```

### Code Quality
```bash
# Format code
black src/

# Linting
flake8 src/

# Type checking (opcional)
mypy src/
```

---

## ğŸ“š DocumentaciÃ³n Adicional

- ğŸ“– [GuÃ­a de InstalaciÃ³n Detallada](docs/SETUP.md)
- ğŸ“Š [ConfiguraciÃ³n de config.yaml](config/config.yaml)
- ğŸ”‘ [Variables de Entorno](config/.env.example)
- ğŸ§ª [Jupyter Notebooks](notebooks/)

---

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

---

## âœ¨ CaracterÃ­sticas Destacadas

- ğŸš€ **Pipeline automatizado end-to-end**
- ğŸ”§ **Altamente configurable** (640+ lÃ­neas de config)
- ğŸŒ **Soporte multilenguaje** (EspaÃ±ol/InglÃ©s)
- ğŸ“Š **MÃºltiples algoritmos de clustering**
- ğŸ¨ **Visualizaciones interactivas** (Plotly)
- ğŸ“„ **Reportes profesionales** en PDF
- ğŸ§ª **Testing completo**
- ğŸ“š **DocumentaciÃ³n exhaustiva**

---

## ğŸ‘¥ Autores

Proyecto de AnÃ¡lisis BibliomÃ©trico - 2025

**Equipo de InvestigaciÃ³n**

---

## ğŸ™ Agradecimientos

Este proyecto utiliza datos de:
- IEEE Xplore Digital Library
- Elsevier Scopus
- Clarivate Web of Science

---

## ğŸ“§ Contacto

Para preguntas, sugerencias o reportar issues:
- ğŸ“¬ Email: [tu-email@example.com]
- ğŸ› Issues: [GitHub Issues](https://github.com/tu-usuario/bibliometric-analysis/issues)

---

**â­ Si este proyecto te resultÃ³ Ãºtil, considera darle una estrella en GitHub!**

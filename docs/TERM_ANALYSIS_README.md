# Sistema de An√°lisis de Frecuencia de T√©rminos

**M√≥dulo:** `src/preprocessing/term_analysis/`
**Fecha:** 2025-10-27

---

## üìã Descripci√≥n

Sistema completo para analizar frecuencia de t√©rminos predefinidos en abstracts cient√≠ficos, con foco en **Concepts of Generative AI in Education**.

### Caracter√≠sticas Principales

‚úÖ **B√∫squeda flexible** con variantes (singular/plural, guiones, espacios)
‚úÖ **An√°lisis de co-ocurrencia** entre t√©rminos
‚úÖ **Estad√≠sticas descriptivas** completas
‚úÖ **3 tipos de visualizaciones** autom√°ticas
‚úÖ **Reportes detallados** en Markdown

---

## üéØ T√©rminos Predefinidos

El sistema analiza estos 15 t√©rminos relacionados con IA Generativa en Educaci√≥n:

1. Generative models
2. Prompting
3. Machine learning
4. Multimodality
5. Fine-tuning
6. Training data
7. Algorithmic bias
8. Explainability
9. Transparency
10. Ethics
11. Privacy
12. Personalization
13. Human-AI interaction
14. AI literacy
15. Co-creation

---

## üîß Componentes Implementados

### 1. PredefinedTermsAnalyzer

Clase principal que implementa todo el an√°lisis.

**Ubicaci√≥n:** `src/preprocessing/term_analysis/predefined_terms_analyzer.py`

#### M√©todos Principales

##### `__init__(unified_data_path)`
Inicializa el analizador cargando abstracts desde JSON.

```python
from src.preprocessing.term_analysis import PredefinedTermsAnalyzer

analyzer = PredefinedTermsAnalyzer('data/unified_articles.json')
```

##### `preprocess_text(text)`
Preprocesamiento suave que preserva t√©rminos compuestos:
- Lowercase para b√∫squeda case-insensitive
- Normalizaci√≥n de espacios
- Mantiene guiones y caracteres especiales

**Ejemplo:**
```python
text = "Machine   Learning and Fine-Tuning"
processed = analyzer.preprocess_text(text)
# Output: "machine learning and fine-tuning"
```

##### `find_term_variants(term)`
Genera variantes del t√©rmino para b√∫squeda flexible.

**Estrategias:**
1. **Singular/Plural:** "models" ‚Üí ["model", "models"]
2. **Guiones:** "Fine-tuning" ‚Üí ["fine-tuning", "fine tuning", "finetuning"]
3. **Formas verbales:** "Fine-tuning" ‚Üí ["finetune", "finetuned"]

**Ejemplo:**
```python
variants = analyzer.find_term_variants("Fine-tuning")
# Output: [
#     "fine-tuning",
#     "fine tuning",
#     "finetuning",
#     "finetune",
#     "finetuned"
# ]
```

##### `calculate_frequencies(abstracts)`
Calcula frecuencias de todos los t√©rminos predefinidos.

**Retorna:**
```python
{
    'Generative models': {
        'total_count': 45,           # Total de ocurrencias
        'documents_count': 23,       # Documentos que lo contienen
        'avg_per_document': 1.96,    # Promedio por documento
        'document_frequency': 0.23,  # % de documentos
        'variants_found': {
            'generative model': 30,
            'generative models': 15
        }
    },
    ...
}
```

**Ejemplo:**
```python
frequencies = analyzer.calculate_frequencies()

# Acceder a datos de un t√©rmino
ml_stats = frequencies['Machine learning']
print(f"Ocurrencias: {ml_stats['total_count']}")
print(f"En {ml_stats['documents_count']} documentos")
```

##### `calculate_cooccurrence_matrix(abstracts)`
Calcula matriz de co-ocurrencia entre t√©rminos.

**Retorna:** DataFrame (t√©rminos √ó t√©rminos) con conteos.

**Ejemplo:**
```python
cooccurrence = analyzer.calculate_cooccurrence_matrix()

# Cu√°ntas veces "Machine learning" y "Ethics" aparecen juntos
count = cooccurrence.loc['Machine learning', 'Ethics']
print(f"Co-ocurrencia: {count} documentos")
```

##### `generate_statistics_report(frequencies)`
Genera DataFrame con estad√≠sticas descriptivas.

**Columnas:**
- Rank
- Term
- Total Count
- Documents
- Avg per Doc
- Doc Frequency (%)
- Variants Used

**Ejemplo:**
```python
stats_df = analyzer.generate_statistics_report(frequencies)
print(stats_df.head())
```

##### `visualize_frequencies(frequencies, output_dir)`
Genera 3 visualizaciones:

1. **Gr√°fico de barras horizontal**
   - Frecuencia total por t√©rmino
   - Colores seg√∫n magnitud
   - Valores anotados

2. **Heatmap de co-ocurrencia**
   - Matriz sim√©trica
   - T√©rminos que aparecen juntos
   - Anotaciones con conteos

3. **Distribuci√≥n estad√≠stica**
   - Histograma de frecuencias
   - Box plot de document frequency
   - Estad√≠sticas descriptivas

**Ejemplo:**
```python
analyzer.visualize_frequencies(
    frequencies,
    'output/term_analysis'
)
# Genera:
#   - term_frequencies_bar.png
#   - term_cooccurrence_heatmap.png
#   - term_distribution_stats.png
```

##### `generate_detailed_report(frequencies, output_path)`
Genera reporte Markdown completo.

**Contenido:**
- Resumen ejecutivo
- Tabla de estad√≠sticas
- Detalles de variantes por t√©rmino
- Insights autom√°ticos

**Ejemplo:**
```python
analyzer.generate_detailed_report(
    frequencies,
    'output/term_analysis/report.md'
)
```

---

## üöÄ Uso R√°pido

### Instalaci√≥n

```bash
pip install numpy pandas matplotlib seaborn scipy tabulate
```

### Demo Completo

```bash
python examples/term_analysis_demo.py
```

### Uso Program√°tico

```python
from src.preprocessing.term_analysis import PredefinedTermsAnalyzer

# 1. Inicializar
analyzer = PredefinedTermsAnalyzer('data/unified_articles.json')

# 2. Calcular frecuencias
frequencies = analyzer.calculate_frequencies()

# 3. Generar estad√≠sticas
stats_df = analyzer.generate_statistics_report(frequencies)
print(stats_df)

# 4. Visualizaciones
analyzer.visualize_frequencies(frequencies, 'output/term_analysis')

# 5. Reporte detallado
analyzer.generate_detailed_report(frequencies, 'output/report.md')
```

---

## üìä Ejemplo de Resultados

### Estad√≠sticas (con datos de ejemplo)

```
Rank  Term                    Total Count  Documents  Avg per Doc  Doc Frequency (%)
----  ----------------------  -----------  ---------  -----------  -----------------
   1  Machine learning               42         35         1.20              70.0
   2  Ethics                         28         22         0.80              44.0
   3  Generative models              25         18         0.71              36.0
   4  Privacy                        20         15         0.57              30.0
   5  Transparency                   18         14         0.51              28.0
   6  Fine-tuning                    15         12         0.43              24.0
   7  Training data                  12         10         0.34              20.0
   8  Personalization                10          8         0.29              16.0
   9  Explainability                  8          7         0.23              14.0
  10  AI literacy                     6          5         0.17              10.0
  11  Human-AI interaction            5          4         0.14               8.0
  12  Multimodality                   4          3         0.11               6.0
  13  Prompting                       3          3         0.09               6.0
  14  Algorithmic bias                2          2         0.06               4.0
  15  Co-creation                     1          1         0.03               2.0
```

### Variantes Detectadas

**Machine learning:**
- `machine learning`: 30 ocurrencias
- `ml`: 8 ocurrencias
- `machine learned`: 4 ocurrencias

**Fine-tuning:**
- `fine-tuning`: 8 ocurrencias
- `finetuning`: 4 ocurrencias
- `fine tuning`: 3 ocurrencias

### Co-ocurrencia (Top pares)

1. `Machine learning` + `Ethics`: 15 documentos
2. `Privacy` + `Ethics`: 12 documentos
3. `Machine learning` + `Generative models`: 10 documentos
4. `Transparency` + `Explainability`: 8 documentos
5. `Training data` + `Machine learning`: 7 documentos

---

## üîç Detalles de Implementaci√≥n

### B√∫squeda con Word Boundaries

Para evitar matches parciales, se usa regex con word boundaries:

```python
# Evita matchear "model" en "remodel"
pattern = r'\b' + re.escape(variant) + r'\b'
matches = re.findall(pattern, abstract)
```

### Generaci√≥n de Variantes

Algoritmo inteligente que genera variantes relevantes:

1. **Guiones:** Sustituir por espacio o eliminar
2. **Singular/Plural:** Reglas heur√≠sticas
3. **T√©rminos compuestos:** Variantes de cada palabra
4. **Formas verbales:** Para t√©rminos como "tuning"

### Cach√© de Variantes

Para optimizar, las variantes se calculan una vez y se cachean:

```python
self._variant_cache = {}

def find_term_variants(self, term):
    if term in self._variant_cache:
        return self._variant_cache[term]
    # ... calcular ...
    self._variant_cache[term] = variants
    return variants
```

---

## üìà Visualizaciones Generadas

### 1. Gr√°fico de Barras

![Term Frequencies Bar](../output/term_analysis/term_frequencies_bar.png)

**Caracter√≠sticas:**
- Barras horizontales ordenadas por frecuencia
- Colores graduados (viridis colormap)
- Valores anotados al final de cada barra
- Grid para mejor lectura

### 2. Heatmap de Co-ocurrencia

![Co-occurrence Heatmap](../output/term_analysis/term_cooccurrence_heatmap.png)

**Caracter√≠sticas:**
- Matriz sim√©trica
- Valores anotados en cada celda
- Escala de colores (YlOrRd)
- Diagonal muestra auto-ocurrencia

### 3. Distribuci√≥n Estad√≠stica

![Distribution Stats](../output/term_analysis/term_distribution_stats.png)

**Caracter√≠sticas:**
- Subplot 1: Histograma con media/mediana
- Subplot 2: Box plot con estad√≠sticas
- Anotaciones informativas

---

## üß™ Testing

### Casos de Prueba

El sistema maneja correctamente:

‚úÖ **Textos vac√≠os**
```python
analyzer.calculate_frequencies([])  # Retorna diccionario vac√≠o
```

‚úÖ **T√©rminos no encontrados**
```python
# T√©rmino con 0 ocurrencias tiene total_count=0
```

‚úÖ **Variantes m√∫ltiples**
```python
# Detecta todas las formas del t√©rmino
```

‚úÖ **Case insensitivity**
```python
# "Machine Learning" == "machine learning"
```

### Ejecutar Demo

```bash
python examples/term_analysis_demo.py
```

**Salida esperada:**
- Log detallado del proceso
- 3 visualizaciones en PNG
- Reporte Markdown
- Estad√≠sticas en consola

---

## üí° Extensibilidad

### Agregar Nuevos T√©rminos

Modificar la lista en la clase:

```python
class PredefinedTermsAnalyzer:
    PREDEFINED_TERMS = [
        "Generative models",
        ...
        "Tu nuevo t√©rmino"  # Agregar aqu√≠
    ]
```

### Personalizar Variantes

Sobrescribir `find_term_variants()`:

```python
class CustomAnalyzer(PredefinedTermsAnalyzer):
    def find_term_variants(self, term):
        variants = super().find_term_variants(term)
        # Agregar l√≥gica personalizada
        variants.extend(['variant_custom1', 'variant_custom2'])
        return variants
```

### Filtrar Abstracts

Analizar solo un subconjunto:

```python
# Filtrar por a√±o
filtered_abstracts = [
    art['abstract']
    for art in analyzer.unified_data
    if art.get('year') == 2023
]

frequencies = analyzer.calculate_frequencies(filtered_abstracts)
```

---

## üìù Outputs Generados

### Archivos de Visualizaci√≥n

```
output/term_analysis/
‚îú‚îÄ‚îÄ term_frequencies_bar.png        # 12x10 inches, 300 DPI
‚îú‚îÄ‚îÄ term_cooccurrence_heatmap.png   # 14x12 inches, 300 DPI
‚îî‚îÄ‚îÄ term_distribution_stats.png     # 14x6 inches, 300 DPI
```

### Reporte Markdown

```
output/term_analysis/
‚îî‚îÄ‚îÄ predefined_terms_report.md      # Reporte completo
```

**Contenido del reporte:**
- Resumen ejecutivo
- Tabla de estad√≠sticas
- Variantes detectadas por t√©rmino
- Insights autom√°ticos

### Logs

```
term_analysis_demo.log              # Log de ejecuci√≥n
```

---

## ‚ö° Optimizaciones Implementadas

### 1. Preprocesamiento √∫nico
```python
# Preprocesar todos los abstracts una vez
preprocessed = [preprocess_text(abs) for abs in abstracts]

# Usar versiones preprocesadas en loops
for abstract in preprocessed:
    # ...
```

### 2. Cach√© de variantes
```python
# Evitar recalcular variantes
self._variant_cache[term] = variants
```

### 3. Regex compilado impl√≠citamente
```python
# re.findall() cachea patterns internamente
pattern = r'\b' + re.escape(variant) + r'\b'
```

---

## üêõ Troubleshooting

### Error: "ModuleNotFoundError: No module named 'tabulate'"

**Soluci√≥n:**
```bash
pip install tabulate
```

### Error: "FileNotFoundError: data/unified_articles.json"

**Soluci√≥n:**
- Verificar que el archivo existe
- Ajustar la ruta si es necesario

### Visualizaciones no se generan

**Causa:** Directorio de salida no existe

**Soluci√≥n:**
```python
Path(output_dir).mkdir(parents=True, exist_ok=True)
```

### T√©rminos no detectados

**Causa:** Variantes no incluidas

**Soluci√≥n:**
- Verificar con `find_term_variants()` qu√© variantes se buscan
- Agregar variantes manualmente si es necesario

---

## üìö Referencias

### Papers Relacionados

- Salton, G., & McGill, M. J. (1983). *Introduction to modern information retrieval.* McGraw-Hill.
- Manning, C. D., et al. (2008). *Introduction to Information Retrieval.* Cambridge University Press.

### Herramientas Utilizadas

- **NumPy:** Operaciones num√©ricas
- **Pandas:** DataFrames y estad√≠sticas
- **Matplotlib:** Visualizaciones base
- **Seaborn:** Visualizaciones avanzadas (heatmaps)
- **Scipy:** Estad√≠sticas adicionales
- **Tabulate:** Formateo de tablas Markdown

---

## üéì Conclusi√≥n

Este sistema proporciona un an√°lisis exhaustivo de t√©rminos predefinidos con:
- ‚úÖ B√∫squeda flexible y robusta
- ‚úÖ Estad√≠sticas descriptivas completas
- ‚úÖ Visualizaciones profesionales
- ‚úÖ Reportes autom√°ticos
- ‚úÖ C√≥digo documentado y extensible

**Estado:** ‚úÖ **COMPLETO Y FUNCIONAL**

---

**√öltima actualizaci√≥n:** 2025-10-27

# GuÃ­a de Preprocesamiento para Clustering

## REQUERIMIENTO 4 - PARTE 1: Preprocesamiento Especializado

DocumentaciÃ³n completa del mÃ³dulo `ClusteringPreprocessor` para preprocesamiento optimizado de textos cientÃ­ficos destinados a clustering jerÃ¡rquico.

---

## Tabla de Contenidos

1. [DescripciÃ³n General](#descripciÃ³n-general)
2. [CaracterÃ­sticas Principales](#caracterÃ­sticas-principales)
3. [InstalaciÃ³n y Requisitos](#instalaciÃ³n-y-requisitos)
4. [Arquitectura del MÃ³dulo](#arquitectura-del-mÃ³dulo)
5. [Uso BÃ¡sico](#uso-bÃ¡sico)
6. [MÃ©todos Detallados](#mÃ©todos-detallados)
7. [MÃ©todos de VectorizaciÃ³n](#mÃ©todos-de-vectorizaciÃ³n)
8. [Pipeline Completo](#pipeline-completo)
9. [Ejemplos Avanzados](#ejemplos-avanzados)
10. [OptimizaciÃ³n y Performance](#optimizaciÃ³n-y-performance)
11. [Troubleshooting](#troubleshooting)

---

## DescripciÃ³n General

El mÃ³dulo `ClusteringPreprocessor` proporciona una soluciÃ³n robusta y optimizada para preparar textos cientÃ­ficos (abstracts, papers) para algoritmos de clustering. Implementa un pipeline completo desde texto crudo hasta vectores numÃ©ricos listos para clustering.

### Problema que Resuelve

Los abstracts cientÃ­ficos contienen:
- **Ruido**: URLs, DOIs, emails, nÃºmeros irrelevantes
- **Stopwords acadÃ©micas**: "paper", "study", "research", "author"
- **Variaciones morfolÃ³gicas**: "models" vs "model", "generated" vs "generate"
- **TÃ©rminos genÃ©ricos**: Que no aportan informaciÃ³n semÃ¡ntica

El preprocesador elimina este ruido mientras **preserva tÃ©rminos tÃ©cnicos crÃ­ticos** de IA/ML/NLP.

### FilosofÃ­a de DiseÃ±o

1. **Limpieza profunda** sin perder informaciÃ³n semÃ¡ntica
2. **PreservaciÃ³n de tÃ©rminos tÃ©cnicos** (neural, transformer, attention)
3. **Eficiencia computacional** (regex compilados, spaCy optimizado)
4. **Flexibilidad** en mÃ©todos de vectorizaciÃ³n
5. **Logging detallado** para debugging y anÃ¡lisis

---

## CaracterÃ­sticas Principales

### 1. Limpieza Profunda (`deep_clean`)

- âœ… ConversiÃ³n a minÃºsculas
- âœ… EliminaciÃ³n de URLs, emails, DOIs
- âœ… NormalizaciÃ³n de nÃºmeros â†’ `TOKEN_NUM`
- âœ… PreservaciÃ³n de guiones en tÃ©rminos compuestos (`multi-task`, `end-to-end`)
- âœ… EliminaciÃ³n de puntuaciÃ³n y caracteres especiales
- âœ… NormalizaciÃ³n de espacios en blanco

### 2. TokenizaciÃ³n Avanzada (`advanced_tokenization`)

- âœ… TokenizaciÃ³n con **spaCy**
- âœ… **POS Tagging**: Filtrado por categorÃ­as gramaticales
  - Mantiene: NOUN, ADJ, VERB, PROPN
  - Elimina: pronombres, artÃ­culos, preposiciones
- âœ… PreservaciÃ³n de **verbos importantes**: generate, train, learn, classify, etc.
- âœ… Filtrado por longitud (min 2 caracteres)

### 3. Stopwords Inteligentes (`remove_stopwords`)

**40+ Stopwords AcadÃ©micas Predefinidas:**
```python
'paper', 'study', 'research', 'article', 'work', 'author',
'propose', 'present', 'show', 'result', 'finding', 'conclusion',
'method', 'approach', 'technique', 'analysis', 'evaluation',
'new', 'novel', 'effective', 'efficient', 'based', 'using'
```

**PreservaciÃ³n de TÃ©rminos TÃ©cnicos:**
```python
# ML/AI
'neural', 'network', 'learning', 'deep', 'machine', 'model',
'training', 'test', 'validation', 'accuracy', 'loss'

# Arquitecturas
'cnn', 'rnn', 'lstm', 'gru', 'transformer', 'attention',
'bert', 'gpt', 'resnet', 'vgg'

# TÃ©cnicas
'classification', 'regression', 'clustering', 'segmentation',
'detection', 'recognition', 'prediction', 'generation'

# Dominios
'vision', 'nlp', 'speech', 'image', 'text', 'language'
```

### 4. LematizaciÃ³n (`lemmatize`)

- âœ… NormalizaciÃ³n morfolÃ³gica con **spaCy**
- âœ… `models` â†’ `model`
- âœ… `generated` â†’ `generate`
- âœ… `training` â†’ `train`
- âœ… EliminaciÃ³n de lemmas invÃ¡lidos (`-PRON-`)

### 5. Tres MÃ©todos de VectorizaciÃ³n

| MÃ©todo | LibrerÃ­a | Dimensionalidad | Uso Recomendado |
|--------|----------|-----------------|-----------------|
| **TF-IDF** | scikit-learn | Configurable (default: 1000) | Clustering rÃ¡pido, interpretable |
| **Word2Vec** | gensim | Configurable (default: 100) | Captura relaciones semÃ¡nticas |
| **SBERT** | sentence-transformers | Fija por modelo (384) | MÃ¡xima calidad semÃ¡ntica |

### 6. Pipeline Completo End-to-End

Un solo mÃ©todo ejecuta todo el flujo:
```python
result = preprocessor.full_preprocessing_pipeline(
    method='tfidf',
    return_intermediate=True
)
```

---

## InstalaciÃ³n y Requisitos

### Dependencias Requeridas

```bash
# BÃ¡sicas (siempre necesarias)
pip install numpy pandas scikit-learn spacy tqdm

# Modelo spaCy
python -m spacy download en_core_web_sm
```

### Dependencias Opcionales

```bash
# Para Word2Vec
pip install gensim

# Para SBERT
pip install sentence-transformers
```

### Versiones Recomendadas

```txt
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
spacy>=3.0.0
tqdm>=4.62.0

# Opcionales
gensim>=4.0.0
sentence-transformers>=2.0.0
```

---

## Arquitectura del MÃ³dulo

```
ClusteringPreprocessor
â”‚
â”œâ”€â”€ __init__(abstracts, spacy_model)
â”‚   â”œâ”€â”€ Carga modelo spaCy
â”‚   â”œâ”€â”€ Optimiza spaCy (desactiva NER)
â”‚   â””â”€â”€ Compila patrones regex
â”‚
â”œâ”€â”€ deep_clean(text) â†’ str
â”‚   â””â”€â”€ Limpieza profunda de texto
â”‚
â”œâ”€â”€ advanced_tokenization(text) â†’ List[str]
â”‚   â””â”€â”€ TokenizaciÃ³n con POS tagging
â”‚
â”œâ”€â”€ remove_stopwords(tokens) â†’ List[str]
â”‚   â””â”€â”€ Filtrado inteligente de stopwords
â”‚
â”œâ”€â”€ lemmatize(tokens) â†’ List[str]
â”‚   â””â”€â”€ NormalizaciÃ³n morfolÃ³gica
â”‚
â”œâ”€â”€ vectorize_texts(texts, method, **kwargs) â†’ (matrix, vectorizer)
â”‚   â”œâ”€â”€ _vectorize_tfidf() â†’ TF-IDF
â”‚   â”œâ”€â”€ _vectorize_word2vec() â†’ Word2Vec embeddings
â”‚   â””â”€â”€ _vectorize_sbert() â†’ SBERT embeddings
â”‚
â””â”€â”€ full_preprocessing_pipeline(method, **kwargs) â†’ Dict
    â””â”€â”€ Ejecuta pipeline completo: clean â†’ tokenize â†’ filter â†’ lemmatize â†’ vectorize
```

---

## Uso BÃ¡sico

### Ejemplo MÃ­nimo

```python
from clustering.clustering_preprocessing import ClusteringPreprocessor

# Preparar abstracts
abstracts = [
    "Deep learning models for image classification using CNNs.",
    "Transformer architectures with attention mechanisms for NLP.",
    "Reinforcement learning agents trained with policy gradients."
]

# Crear preprocesador
preprocessor = ClusteringPreprocessor(abstracts)

# Ejecutar pipeline completo
result = preprocessor.full_preprocessing_pipeline(method='tfidf')

# Acceder a resultados
feature_matrix = result['feature_matrix']  # Matriz numÃ©rica (n_docs x n_features)
vectorizer = result['vectorizer']          # Objeto TfidfVectorizer
processed_texts = result['processed_texts'] # Textos preprocesados

print(f"Matriz: {feature_matrix.shape}")
print(f"Documentos: {result['n_documents']}")
print(f"Features: {result['n_features']}")
```

**Salida esperada:**
```
Matriz: (3, 1000)
Documentos: 3
Features: 1000
```

---

## MÃ©todos Detallados

### 1. `deep_clean(text: str) -> str`

Limpieza profunda de texto cientÃ­fico.

**Ejemplo:**
```python
text = "Check https://example.com for details. Contact author@uni.edu. DOI: 10.1234/paper.2023. We tested 100 samples."

cleaned = preprocessor.deep_clean(text)
print(cleaned)
# Salida: "check for details contact we tested token_num samples"
```

**Transformaciones:**
1. `https://example.com` â†’ (eliminado)
2. `author@uni.edu` â†’ (eliminado)
3. `10.1234/paper.2023` â†’ (eliminado)
4. `100` â†’ `token_num`

### 2. `advanced_tokenization(text: str) -> List[str]`

TokenizaciÃ³n con POS tagging.

**Ejemplo:**
```python
text = "the neural network model is trained on large datasets"

tokens = preprocessor.advanced_tokenization(text)
print(tokens)
# Salida: ['neural', 'network', 'model', 'trained', 'large', 'datasets']
```

**POS Filtering:**
- âœ… NOUN: `network`, `model`, `datasets`
- âœ… ADJ: `neural`, `large`
- âœ… VERB: `trained`
- âŒ DET: `the` (eliminado)
- âŒ ADP: `on` (eliminado)
- âŒ AUX: `is` (eliminado)

### 3. `remove_stopwords(tokens: List[str]) -> List[str]`

Filtrado inteligente de stopwords.

**Ejemplo:**
```python
tokens = ['paper', 'propose', 'neural', 'network', 'study', 'deep', 'learning']

filtered = preprocessor.remove_stopwords(tokens)
print(filtered)
# Salida: ['neural', 'network', 'deep', 'learning']
```

**Criterios:**
- âŒ `paper`, `propose`, `study` â†’ stopwords acadÃ©micas
- âœ… `neural`, `network`, `deep`, `learning` â†’ tÃ©rminos tÃ©cnicos preservados

### 4. `lemmatize(tokens: List[str]) -> List[str]`

NormalizaciÃ³n morfolÃ³gica.

**Ejemplo:**
```python
tokens = ['models', 'training', 'generated', 'networks']

lemmas = preprocessor.lemmatize(tokens)
print(lemmas)
# Salida: ['model', 'train', 'generate', 'network']
```

---

## MÃ©todos de VectorizaciÃ³n

### TF-IDF (Default)

**Uso:**
```python
result = preprocessor.full_preprocessing_pipeline(
    method='tfidf',
    max_features=1000,    # Limitar vocabulario
    ngram_range=(1, 3),   # Unigramas, bigramas, trigramas
    min_df=2,             # MÃ­nimo 2 documentos
    max_df=0.85           # MÃ¡ximo 85% documentos
)
```

**Ventajas:**
- âš¡ Muy rÃ¡pido
- ğŸ“Š Interpretable (palabras importantes tienen scores altos)
- ğŸ”§ Altamente configurable
- ğŸ’¾ Eficiente en memoria (matriz sparse)

**CuÃ¡ndo usar:**
- Corpus grande (>1000 documentos)
- Necesitas interpretabilidad
- Recursos computacionales limitados

### Word2Vec

**Uso:**
```python
result = preprocessor.full_preprocessing_pipeline(
    method='word2vec',
    vector_size=100,      # DimensiÃ³n de embeddings
    window=5,             # Ventana de contexto
    min_count=2,          # Frecuencia mÃ­nima
    epochs=10,            # Iteraciones de entrenamiento
    sg=1                  # 1=Skip-gram, 0=CBOW
)
```

**Ventajas:**
- ğŸ§  Captura relaciones semÃ¡nticas
- ğŸ“ˆ Embeddings densos (no sparse)
- ğŸ” Palabras similares tienen vectores cercanos
- âš™ï¸ Entrena en tu corpus especÃ­fico

**CuÃ¡ndo usar:**
- Necesitas capturar semÃ¡ntica
- Corpus de tamaÃ±o medio (100-10000 docs)
- Clustering basado en similitud semÃ¡ntica

### SBERT (Sentence-BERT)

**Uso:**
```python
result = preprocessor.full_preprocessing_pipeline(
    method='sbert',
    model_name='all-MiniLM-L6-v2',  # Modelo pre-entrenado
    batch_size=32
)
```

**Modelos disponibles:**
- `all-MiniLM-L6-v2` (384 dims, rÃ¡pido)
- `all-mpnet-base-v2` (768 dims, mejor calidad)
- `paraphrase-multilingual-MiniLM-L12-v2` (multilingÃ¼e)

**Ventajas:**
- ğŸ† MÃ¡xima calidad semÃ¡ntica
- ğŸŒ Modelos multilingÃ¼es disponibles
- ğŸ¯ Pre-entrenado en millones de textos
- ğŸ“Š State-of-the-art en similitud textual

**CuÃ¡ndo usar:**
- Necesitas mÃ¡xima calidad
- Corpus pequeÃ±o (<1000 docs)
- Clustering de alta precisiÃ³n

---

## Pipeline Completo

### Flujo de EjecuciÃ³n

```python
result = preprocessor.full_preprocessing_pipeline(
    method='tfidf',
    return_intermediate=True,  # Retorna resultados intermedios
    max_features=1000
)
```

**Pasos ejecutados:**

```
[1/5] Limpieza profunda
  â”œâ”€â”€ URLs, emails, DOIs eliminados
  â”œâ”€â”€ NÃºmeros normalizados
  â””â”€â”€ PuntuaciÃ³n eliminada
  âœ“ 100 textos limpiados

[2/5] TokenizaciÃ³n avanzada
  â”œâ”€â”€ spaCy tokenization
  â”œâ”€â”€ POS filtering
  â””â”€â”€ Total tokens: 5000, Promedio: 50 tokens/doc
  âœ“ TokenizaciÃ³n completada

[3/5] EliminaciÃ³n de stopwords
  â”œâ”€â”€ Stopwords acadÃ©micas filtradas
  â”œâ”€â”€ TÃ©rminos tÃ©cnicos preservados
  â””â”€â”€ Tokens despuÃ©s: 3500 (70%)
  âœ“ Filtrado completado

[4/5] LematizaciÃ³n
  â”œâ”€â”€ NormalizaciÃ³n morfolÃ³gica
  â””â”€â”€ Vocabulario Ãºnico: 800 tÃ©rminos
  âœ“ LematizaciÃ³n completada

[5/5] VectorizaciÃ³n con tfidf
  â”œâ”€â”€ Matriz TF-IDF: (100, 1000)
  â””â”€â”€ Densidad: 15.3%
  âœ“ VectorizaciÃ³n completada
```

### Resultado Completo

```python
result = {
    # Salidas principales
    'feature_matrix': np.ndarray,      # Matriz numÃ©rica (n_docs x n_features)
    'vectorizer': Object,              # TfidfVectorizer / Word2Vec / SBERT
    'processed_texts': List[str],      # Textos preprocesados
    'n_documents': int,                # NÃºmero de documentos
    'n_features': int,                 # NÃºmero de features
    'method': str,                     # MÃ©todo usado ('tfidf', 'word2vec', 'sbert')

    # Resultados intermedios (si return_intermediate=True)
    'cleaned_texts': List[str],        # Textos despuÃ©s de limpieza
    'tokenized_texts': List[List[str]], # Tokens por documento
    'lemmatized_texts': List[List[str]], # Lemmas por documento
    'vocabulary_size': int             # TamaÃ±o del vocabulario
}
```

---

## Ejemplos Avanzados

### Ejemplo 1: Comparar MÃ©todos de VectorizaciÃ³n

```python
from clustering.clustering_preprocessing import ClusteringPreprocessor
import numpy as np

abstracts = [...]  # Tu corpus

preprocessor = ClusteringPreprocessor(abstracts)

# Comparar TF-IDF vs Word2Vec vs SBERT
methods = ['tfidf', 'word2vec', 'sbert']
results = {}

for method in methods:
    try:
        result = preprocessor.full_preprocessing_pipeline(method=method)
        results[method] = result

        print(f"\n{method.upper()}:")
        print(f"  Matriz: {result['feature_matrix'].shape}")
        print(f"  Features: {result['n_features']}")
        print(f"  Densidad: {np.count_nonzero(result['feature_matrix']) / result['feature_matrix'].size * 100:.2f}%")

    except ImportError as e:
        print(f"{method}: {e}")
```

### Ejemplo 2: AnÃ¡lisis de Resultados Intermedios

```python
# Ejecutar con resultados intermedios
result = preprocessor.full_preprocessing_pipeline(
    method='tfidf',
    return_intermediate=True
)

# Analizar transformaciones
print("\n=== ANÃLISIS DE TRANSFORMACIONES ===")

for i, abstract in enumerate(abstracts[:3]):
    print(f"\n[Documento {i+1}]")
    print(f"Original: {abstract[:100]}...")
    print(f"Limpio: {result['cleaned_texts'][i][:100]}...")
    print(f"Tokens: {result['tokenized_texts'][i][:10]}...")
    print(f"Lemmas: {result['lemmatized_texts'][i][:10]}...")
    print(f"Procesado: {result['processed_texts'][i][:100]}...")
```

### Ejemplo 3: CustomizaciÃ³n Completa

```python
# Crear preprocesador
preprocessor = ClusteringPreprocessor(abstracts, spacy_model='en_core_web_sm')

# Procesar paso a paso con customizaciÃ³n
cleaned = [preprocessor.deep_clean(text) for text in abstracts]
tokenized = [preprocessor.advanced_tokenization(text) for text in cleaned]

# AÃ±adir stopwords personalizadas
custom_stopwords = {'dataset', 'datasets', 'experiment', 'experiments'}
filtered = [preprocessor.remove_stopwords(tokens, custom_stopwords) for tokens in tokenized]

# Lematizar
lemmatized = [preprocessor.lemmatize(tokens) for tokens in filtered]

# Vectorizar con parÃ¡metros especÃ­ficos
processed_texts = [' '.join(tokens) for tokens in lemmatized]
matrix, vectorizer = preprocessor.vectorize_texts(
    processed_texts,
    method='tfidf',
    max_features=500,
    ngram_range=(1, 2),
    min_df=3,
    max_df=0.7
)

print(f"Matriz customizada: {matrix.shape}")
```

### Ejemplo 4: Procesamiento por Lotes

```python
import pandas as pd

# Cargar corpus grande
df = pd.read_csv('large_corpus.csv')
abstracts = df['abstract'].tolist()

# Procesar en lotes para memoria limitada
batch_size = 1000
all_results = []

for i in range(0, len(abstracts), batch_size):
    batch = abstracts[i:i+batch_size]

    preprocessor = ClusteringPreprocessor(batch)
    result = preprocessor.full_preprocessing_pipeline(method='tfidf')

    all_results.append(result['feature_matrix'])

    print(f"Lote {i//batch_size + 1} completado: {result['feature_matrix'].shape}")

# Concatenar resultados
final_matrix = np.vstack(all_results)
print(f"Matriz final: {final_matrix.shape}")
```

---

## OptimizaciÃ³n y Performance

### Optimizaciones Implementadas

1. **Regex Pre-compilados**
   ```python
   # En __init__, se compilan una vez
   self.url_pattern = re.compile(r'https?://\S+|www\.\S+')
   self.email_pattern = re.compile(r'\S+@\S+')
   # Reutilizados en cada llamada a deep_clean()
   ```

2. **spaCy Optimizado**
   ```python
   # Desactivar componentes innecesarios
   if 'ner' in self.nlp.pipe_names:
       self.nlp.disable_pipes(['ner'])  # ~20% mÃ¡s rÃ¡pido
   ```

3. **Procesamiento en Batch**
   ```python
   # LematizaciÃ³n en batch
   text = ' '.join(tokens)
   doc = self.nlp(text)  # Una sola llamada
   ```

4. **Progress Bars**
   ```python
   from tqdm import tqdm

   self.cleaned_texts = [
       self.deep_clean(text)
       for text in tqdm(self.abstracts, desc="Limpiando")
   ]
   ```

### Benchmarks

| Corpus Size | Method | Tiempo (s) | Memoria (MB) |
|-------------|--------|------------|--------------|
| 100 docs | TF-IDF | 2.3 | 150 |
| 100 docs | Word2Vec | 5.7 | 220 |
| 100 docs | SBERT | 8.4 | 800 |
| 1000 docs | TF-IDF | 18.5 | 380 |
| 1000 docs | Word2Vec | 52.3 | 950 |
| 1000 docs | SBERT | 64.1 | 2100 |

### Tips de OptimizaciÃ³n

1. **Para corpus grandes**: Usar TF-IDF
   ```python
   result = preprocessor.full_preprocessing_pipeline(
       method='tfidf',
       max_features=500  # Reducir features para velocidad
   )
   ```

2. **Para mÃ¡xima velocidad**: Desactivar logging
   ```python
   import logging
   logging.getLogger('clustering.clustering_preprocessing').setLevel(logging.WARNING)
   ```

3. **Para memoria limitada**: Procesar en lotes
   ```python
   # Ver Ejemplo 4 arriba
   ```

---

## Troubleshooting

### Problema: `OSError: Model 'en_core_web_sm' not found`

**SoluciÃ³n:**
```bash
python -m spacy download en_core_web_sm
```

### Problema: `ImportError: Gensim no estÃ¡ instalado`

**SoluciÃ³n:**
```bash
pip install gensim
```

O usar TF-IDF:
```python
result = preprocessor.full_preprocessing_pipeline(method='tfidf')
```

### Problema: `ImportError: Sentence-Transformers no estÃ¡ instalado`

**SoluciÃ³n:**
```bash
pip install sentence-transformers
```

### Problema: Vocabulario vacÃ­o en TF-IDF

**Causa:** ParÃ¡metros `min_df` / `max_df` muy restrictivos

**SoluciÃ³n:**
```python
result = preprocessor.full_preprocessing_pipeline(
    method='tfidf',
    min_df=1,      # Reducir mÃ­nimo
    max_df=1.0     # Aumentar mÃ¡ximo
)
```

### Problema: Memoria insuficiente con SBERT

**SoluciÃ³n:** Reducir batch_size
```python
result = preprocessor.full_preprocessing_pipeline(
    method='sbert',
    batch_size=8  # Default: 32
)
```

### Problema: Procesamiento muy lento

**Causas posibles:**
1. spaCy con componentes innecesarios activados
2. Corpus muy grande sin batching
3. SBERT en CPU (sin GPU)

**Soluciones:**
```python
# 1. Verificar componentes desactivados
print(preprocessor.nlp.pipe_names)  # No debe incluir 'ner'

# 2. Procesar en lotes (ver Ejemplo 4)

# 3. Usar TF-IDF o Word2Vec en CPU
```

---

## PrÃ³ximos Pasos

DespuÃ©s de completar el preprocesamiento, los siguientes pasos tÃ­picos son:

1. **Clustering JerÃ¡rquico** (PARTE 2)
   - Algoritmos: Agglomerative, Divisive
   - MÃ©tricas de distancia: Euclidean, Cosine
   - Linkage: Ward, Average, Complete

2. **EvaluaciÃ³n de Clusters** (PARTE 3)
   - Silhouette Score
   - Davies-Bouldin Index
   - Calinski-Harabasz Score
   - Dendrogramas

3. **VisualizaciÃ³n** (PARTE 4)
   - Dendrogramas interactivos
   - t-SNE / UMAP para visualizaciÃ³n 2D
   - Heatmaps de similitud
   - Word clouds por cluster

---

## Referencias

- **spaCy**: https://spacy.io/
- **scikit-learn TF-IDF**: https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting
- **Gensim Word2Vec**: https://radimrehurek.com/gensim/models/word2vec.html
- **Sentence-Transformers**: https://www.sbert.net/

---

**Ãšltima actualizaciÃ³n**: 2025
**VersiÃ³n**: 1.0
**Autor**: Sistema de AnÃ¡lisis de TÃ©rminos

"""
Script de Ejecuci√≥n - Portal Institucional Uniquind√≠o
Acceso automatizado a bases de datos acad√©micas

Uso:
    python run_uniquindio_portal.py
    
    O con par√°metros:
    python run_uniquindio_portal.py --query "generative AI" --max-results 50
"""

import sys
import json
from pathlib import Path
from datetime import datetime
from loguru import logger

# Agregar directorio bibliometric-analysis al path
sys.path.insert(0, str(Path(__file__).parent / "bibliometric-analysis"))

from src.utils.config_loader import get_config
from src.scrapers.uniquindio_portal_scraper import UniquindioPortalScraper


def setup_logging():
    """Configurar logging"""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"uniquindio_portal_{timestamp}.log"
    
    logger.add(
        log_file,
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {message}",
        level="INFO"
    )


def save_results(results: dict, output_dir: Path):
    """
    Guardar resultados en archivos JSON
    
    Args:
        results: Diccionario con resultados del scraping
        output_dir: Directorio de salida
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Guardar resultados completos
    full_results_file = output_dir / f"uniquindio_full_results_{timestamp}.json"
    with open(full_results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"‚úÖ Resultados completos guardados en: {full_results_file}")
    
    # Guardar art√≠culos por base de datos
    for db_name, db_data in results.get('databases', {}).items():
        db_file = output_dir / f"uniquindio_{db_name.lower().replace(' ', '_')}_{timestamp}.json"
        
        db_export = {
            "database": db_name,
            "query": results['query'],
            "timestamp": results['timestamp'],
            "count": db_data['count'],
            "articles": db_data['articles']
        }
        
        with open(db_file, 'w', encoding='utf-8') as f:
            json.dump(db_export, f, indent=2, ensure_ascii=False)
        
        logger.info(f"  üìÑ {db_name}: {db_file.name}")
    
    # Crear resumen
    summary = {
        "execution_date": datetime.now().isoformat(),
        "query": results['query'],
        "portal": results['portal'],
        "status": results['status'],
        "statistics": {
            "total_articles": results['total_records'],
            "databases_processed": len(results.get('databases', {})),
            "databases": {
                db_name: db_data['count']
                for db_name, db_data in results.get('databases', {}).items()
            }
        }
    }
    
    summary_file = output_dir / f"uniquindio_summary_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    logger.info(f"‚úÖ Resumen guardado en: {summary_file}")
    
    return full_results_file


def print_summary(results: dict):
    """Imprimir resumen de resultados"""
    print("\n" + "="*80)
    print("üìä RESUMEN DE EXTRACCI√ìN")
    print("="*80)
    print(f"üîç Query: {results['query']}")
    print(f"üåê Portal: {results['portal']}")
    print(f"üìÖ Fecha: {results['timestamp']}")
    print(f"üìà Total de art√≠culos: {results['total_records']}")
    print(f"üéØ Estado: {results['status']}")
    print("\nüìö Art√≠culos por base de datos:")
    
    for db_name, db_data in results.get('databases', {}).items():
        print(f"  ‚Ä¢ {db_name}: {db_data['count']} art√≠culos")
    
    print("="*80)


def main():
    """Funci√≥n principal"""
    import argparse
    import os
    
    # Cambiar directorio de trabajo a bibliometric-analysis
    os.chdir(Path(__file__).parent / "bibliometric-analysis")
    
    parser = argparse.ArgumentParser(
        description="Scraper del portal institucional Uniquind√≠o"
    )
    parser.add_argument(
        "--query",
        type=str,
        default="generative artificial intelligence",
        help="T√©rmino de b√∫squeda (default: 'generative artificial intelligence')"
    )
    parser.add_argument(
        "--max-results",
        type=int,
        default=50,
        help="N√∫mero m√°ximo de resultados por base de datos (default: 50)"
    )
    parser.add_argument(
        "--headless",
        action="store_true",
        help="Ejecutar navegador en modo headless (sin interfaz gr√°fica)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/raw",
        help="Directorio de salida (default: 'data/raw')"
    )
    
    args = parser.parse_args()
    
    # Configurar logging
    setup_logging()
    
    print("\n" + "="*80)
    print("üéì PORTAL INSTITUCIONAL - UNIVERSIDAD DEL QUIND√çO")
    print("="*80)
    print(f"üîç Query: {args.query}")
    print(f"üìä Resultados m√°ximos por BD: {args.max_results}")
    print(f"üåê Portal: https://library.uniquindio.edu.co/databases")
    print("="*80 + "\n")
    
    # Cargar configuraci√≥n
    logger.info("Cargando configuraci√≥n...")
    config = get_config()
    
    # Inicializar scraper
    logger.info("Inicializando scraper del portal institucional...")
    scraper = UniquindioPortalScraper(config, headless=args.headless)
    
    try:
        # Ejecutar scraping
        logger.info("üöÄ Iniciando extracci√≥n de datos...")
        results = scraper.scrape(
            query=args.query,
            max_results=args.max_results
        )
        
        # Verificar si hay resultados
        if results['total_records'] == 0:
            logger.warning("‚ö†Ô∏è No se obtuvieron resultados")
            print("\n‚ö†Ô∏è No se encontraron art√≠culos.")
            print("\nüí° Posibles razones:")
            print("  1. No est√°s conectado a la red institucional de Uniquind√≠o")
            print("  2. Necesitas configurar VPN institucional")
            print("  3. Las bases de datos requieren autenticaci√≥n manual")
            print("\nüìå Recomendaciones:")
            print("  ‚Ä¢ Con√©ctate a la red de la universidad")
            print("  ‚Ä¢ O configura la VPN institucional")
            print("  ‚Ä¢ Ejecuta sin --headless para autenticarte manualmente")
            return
        
        # Guardar resultados
        output_dir = Path(args.output_dir)
        results_file = save_results(results, output_dir)
        
        # Mostrar resumen
        print_summary(results)
        
        print(f"\n‚úÖ Extracci√≥n completada exitosamente")
        print(f"üìÅ Resultados guardados en: {output_dir}")
        print(f"üìÑ Archivo principal: {results_file.name}")
        
        # Siguiente paso
        print("\n" + "="*80)
        print("üìå PR√ìXIMO PASO: UNIFICACI√ìN DE DATOS")
        print("="*80)
        print("\nPara unificar los datos descargados, ejecuta:")
        print("  python automation_pipeline.py")
        print("\nO usa el men√∫ interactivo:")
        print("  python menu_interactivo.py")
        print("="*80 + "\n")
    
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è Ejecuci√≥n interrumpida por el usuario")
        print("\n\n‚ö†Ô∏è Proceso cancelado por el usuario")
    
    except Exception as e:
        logger.error(f"‚ùå Error en la ejecuci√≥n: {e}")
        print(f"\n‚ùå Error: {e}")
        print("\nüí° Sugerencias:")
        print("  ‚Ä¢ Verifica tu conexi√≥n a la red institucional")
        print("  ‚Ä¢ Aseg√∫rate de tener ChromeDriver instalado")
        print("  ‚Ä¢ Revisa los logs en la carpeta 'logs/'")
    
    finally:
        logger.info("Finalizando scraper...")


if __name__ == "__main__":
    main()
